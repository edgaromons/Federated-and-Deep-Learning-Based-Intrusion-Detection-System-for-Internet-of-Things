# -*- coding: utf-8 -*-
"""Copy_of_Federated_and_Deep_Learning_Based_Intrusion_Detection_System_for_Internet_of_Things_Implementation_Using_Raspberry_Pi (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nmLBm7vHCOu2eO9-QlYm5UPM8NxnG6cB

#**Federated and Deep Learning Based Intrusion Detection System for Internet of Things (Implementation Using Raspberry Pi)**
"""

os.getcwd()

today = date.today()

"""
   Obtain the N-BaIoT Dataset from kaggle website:
"""

filepath = "https://www.kaggle.com/datasets/mkashifn/nbaiot-dataset"
od.download(filepath)

# Load Dataset for IoT Device 1:
benign = pd.read_csv('/content/nbaiot-dataset/1.benign.csv')
benign.shape

# Botnet --> Mirai
# Attack --> Scan volnerable devices / Ack flooding / Syn flooding / UDP flooding / UDPplain

m_scan = pd.read_csv('/content/nbaiot-dataset/1.mirai.scan.csv')
m_ack = pd.read_csv('/content/nbaiot-dataset/1.mirai.ack.csv')
m_syn = pd.read_csv('/content/nbaiot-dataset/1.mirai.syn.csv')
m_udp = pd.read_csv('/content/nbaiot-dataset/1.mirai.udp.csv')
m_udpplain = pd.read_csv('/content/nbaiot-dataset/1.mirai.udpplain.csv')

#Botnet --> Gafgyt (BASHLITE)
#Attack --> Scan vulnerable devices / Sending spam / UDP flooding / TCP flooding / Sending spam & opening connection
g_scan = pd.read_csv('/content/nbaiot-dataset/1.gafgyt.scan.csv')
g_junk = pd.read_csv('/content/nbaiot-dataset/1.gafgyt.junk.csv')
g_udp = pd.read_csv('/content/nbaiot-dataset/1.gafgyt.udp.csv')
g_tcp = pd.read_csv('/content/nbaiot-dataset/1.gafgyt.tcp.csv')
g_combo = pd.read_csv('/content/nbaiot-dataset/1.gafgyt.combo.csv')

#Convert into one dataset
# add new column
benign['type'] = 'benign'
m_scan['type'] = 'm_scan'
m_ack['type'] = 'm_ack'
m_syn['type']  = 'm_syn'
m_udp['type'] = 'm_udp'
m_udpplain['type'] = 'm_udpplain'
g_combo['type'] = 'g_combo'
g_junk['type'] = 'g_junk'
g_scan['type'] = 'g_scan'
g_tcp['type'] = 'g_tcp'
g_udp['type'] = 'g_udp'

d = pd.concat([benign,m_scan,m_ack,m_syn,m_udp,m_udpplain,g_combo,g_junk,g_scan,g_tcp,g_udp], axis=0, sort=False, ignore_index=True)
d.shape

#value counts
labels = d['type'].value_counts()
label_count = pd.DataFrame({'labels': labels.index,'counts': labels.values})

# Plot the IoT Device 1 Data:

labels = label_count['labels']
sizes = label_count['counts']
plt.figure(figsize=(8,6))
colors = ['red', 'orange', 'yellow', 'green', 'blue', 'lightskyblue', 'pink', 'purple', 'rosybrown', 'olive', 'darkgrey']
# colors = [1,2,3,4,5,6,7,8,9,10,11]
explode = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1)
patches, texts = plt.pie(sizes, explode = explode, colors=colors, shadow=True, startangle=100, pctdistance=100, labeldistance=10)
plt.legend(patches, labels, loc="best")
plt.axis('equal')
plt.tight_layout()
plt.title('IoT Device 1 Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 26, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('IoT Device 1 Showing the Normal-Benign and Malicious Attacks in a Pie Chart_1A.png')
plt.show()

# Save the dataset
d.to_csv('/content/Device #1.csv')

# Load the data
d1 = pd.read_csv('/content/Device #1.csv')

"""
   Visualize the Dataset for IoT Device 1:
"""
print(d1.info())
print(d1.shape)
print(d1.describe())
d1

# Dataset for IoT Device 1 Classes:
d1['type'].value_counts()

"""
   Exploratory Data Analysis (EDA): - Dataset for IoT Device 1
"""
## Plot the data:

plt.figure(figsize = (20, 10))
my_circle = plt.Circle((0, 0), 0.7, color='white')
plt.pie(d1['type'].value_counts(), labels=['Mirai Udp attack', 'Mirai Syn attack', 'Mirai Scan attack', 'Gafgyt Udp attack', 'Mirai Ack attack', 'Gafgyt Tcp attack',
                                                   'Mirai udpplain', 'Gafgyt Combo attack', 'Normal Traffic data', 'Gafgyt Scan attack', 'Gafgyt Junk attack'],
        colors=['red', 'green', 'blue', 'orange', 'yellow', 'pink', 'olive', 'darkgrey', 'sienna', 'purple', 'rosybrown'], autopct = '%1.1f%%', textprops={'fontsize': 17, })
p = plt.gcf()
p.gca().add_artist(my_circle)
plt.title('IoT Device 1 Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 26, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('IoT Device 1 Showing the Normal-Benign and Malicious Attacks in a Pie Chart_1B.png')
plt.show()

# Load Dataset for IoT Device 2:
benign = pd.read_csv('/content/nbaiot-dataset/2.benign.csv')
benign.shape

# Botnet --> Mirai
# Attack --> Scan volnerable devices / Ack flooding / Syn flooding / UDP flooding / UDPplain

m_scan = pd.read_csv('/content/nbaiot-dataset/2.mirai.scan.csv')
m_ack = pd.read_csv('/content/nbaiot-dataset/2.mirai.ack.csv')
m_syn = pd.read_csv('/content/nbaiot-dataset/2.mirai.syn.csv')
m_udp = pd.read_csv('/content/nbaiot-dataset/2.mirai.udp.csv')
m_udpplain = pd.read_csv('/content/nbaiot-dataset/2.mirai.udpplain.csv')

#Botnet --> Gafgyt (BASHLITE)
#Attack --> Scan vulnerable devices / Sending spam / UDP flooding / TCP flooding / Sending spam & opening connection
g_scan = pd.read_csv('/content/nbaiot-dataset/2.gafgyt.scan.csv')
g_junk = pd.read_csv('/content/nbaiot-dataset/2.gafgyt.junk.csv')
g_udp = pd.read_csv('/content/nbaiot-dataset/2.gafgyt.udp.csv')
g_tcp = pd.read_csv('/content/nbaiot-dataset/2.gafgyt.tcp.csv')
g_combo = pd.read_csv('/content/nbaiot-dataset/2.gafgyt.combo.csv')

##Convert into one dataset
# add new column
benign['type'] = 'benign'
m_scan['type'] = 'm_scan'
m_ack['type'] = 'm_ack'
m_syn['type']  = 'm_syn'
m_udp['type'] = 'm_udp'
m_udpplain['type'] = 'm_udpplain'
g_combo['type'] = 'g_combo'
g_junk['type'] = 'g_junk'
g_scan['type'] = 'g_scan'
g_tcp['type'] = 'g_tcp'
g_udp['type'] = 'g_udp'

d = pd.concat([benign,m_scan,m_ack,m_syn,m_udp,m_udpplain,g_combo,g_junk,g_scan,g_tcp,g_udp], axis=0, sort=False, ignore_index=True)
d.shape

#value counts
labels = d['type'].value_counts()
label_count = pd.DataFrame({'labels': labels.index,'counts': labels.values})

#Plot the Data:
labels = label_count['labels']
sizes = label_count['counts']
plt.figure(figsize=(8,6))
colors = ['red', 'orange', 'yellow', 'green', 'blue', 'lightskyblue', 'pink', 'purple', 'rosybrown', 'olive', 'darkgrey']
# colors = [1,2,3,4,5,6,7,8,9,10,11]
explode = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1)
patches, texts = plt.pie(sizes, explode = explode, colors=colors, shadow=True, startangle=100, pctdistance=100, labeldistance=10)
plt.legend(patches, labels, loc="best")
plt.axis('equal')
plt.tight_layout()
plt.title('IoT Device 2 Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 26, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('IoT Device 2 Showing the Normal-Benign and Malicious Attacks in a Pie Chart_2A.png')
plt.show()

# Save the dataset
d.to_csv('/content/Device #2.csv')

# Load the data
d2 = pd.read_csv('/content/Device #2.csv')

"""
   Visualize the Dataset for IoT Device 2:
"""
print(d2.info())
print(d2.shape)
print(d2.describe())
d2

# Dataset for IoT Device 2 Classes:
d2['type'].value_counts()

"""
   Exploratory Data Analysis (EDA): - Dataset for IoT Device 2
"""
## Plot the data:

plt.figure(figsize = (20, 10))
my_circle = plt.Circle((0, 0), 0.7, color='white')
plt.pie(d2['type'].value_counts(), labels=['Mirai Udp attack', 'Mirai Syn attack', 'Mirai Scan attack', 'Gafgyt Udp attack', 'Mirai Ack attack', 'Gafgyt Tcp attack',
                                                   'Mirai udpplain', 'Gafgyt Combo attack', 'Normal Traffic data', 'Gafgyt Scan attack', 'Gafgyt Junk attack'],
        colors=['red', 'green', 'blue', 'orange', 'yellow', 'pink', 'olive', 'darkgrey', 'sienna', 'purple', 'rosybrown'], autopct = '%1.1f%%', textprops={'fontsize': 17, })
p = plt.gcf()
p.gca().add_artist(my_circle)
plt.title('IoT Device 2 Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 26, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('IoT Device 2 Showing the Normal-Benign and Malicious Attacks in a Pie Chart_2B.png')
plt.show()

# Load Dataset for IoT Device 3:
benign = pd.read_csv('/content/nbaiot-dataset/3.benign.csv')
benign.shape

#Botnet --> Gafgyt (BASHLITE)
#Attack --> Scan vulnerable devices / Sending spam / UDP flooding / TCP flooding / Sending spam & opening connection
g_scan = pd.read_csv('/content/nbaiot-dataset/3.gafgyt.scan.csv')
g_junk = pd.read_csv('/content/nbaiot-dataset/3.gafgyt.junk.csv')
g_udp = pd.read_csv('/content/nbaiot-dataset/3.gafgyt.udp.csv')
g_tcp = pd.read_csv('/content/nbaiot-dataset/3.gafgyt.tcp.csv')
g_combo = pd.read_csv('/content/nbaiot-dataset/3.gafgyt.combo.csv')

##Convert into one dataset
# add new column
benign['type'] = 'benign'
g_combo['type'] = 'g_combo'
g_junk['type'] = 'g_junk'
g_scan['type'] = 'g_scan'
g_tcp['type'] = 'g_tcp'
g_udp['type'] = 'g_udp'

d = pd.concat([benign,g_combo,g_junk,g_scan,g_tcp,g_udp], axis=0, sort=False, ignore_index=True)
d.shape

#value counts
labels = d['type'].value_counts()
label_count = pd.DataFrame({'labels': labels.index,'counts': labels.values})

# Plot the data:
labels = label_count['labels']
sizes = label_count['counts']
plt.figure(figsize=(8,6))
colors = ['red', 'orange', 'yellow', 'green', 'olive', 'darkgrey']
# colors = [1,2,3,4,5,6,7,8,9,10,11]
explode = (0, 0, 0, 0.1, 0, 0)
patches, texts = plt.pie(sizes, explode = explode, colors=colors, shadow=True, startangle=100, pctdistance=100, labeldistance=10)
plt.legend(patches, labels, loc="best")
plt.axis('equal')
plt.tight_layout()
plt.title('IoT Device 3 Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 26, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('IoT Device 3 Showing the Normal-Benign and Malicious Attacks in a Pie Chart_3A.png')
plt.show()

# Save the dataset
d.to_csv('/content/Device #3.csv')

# Load the data
d3 = pd.read_csv('/content/Device #3.csv')

"""
   Visualize the Dataset for IoT Device 3:
"""
print(d3.info())
print(d3.shape)
print(d3.describe())
d3

# Dataset for IoT Device 3 Classes:
d3['type'].value_counts()

"""
   Exploratory Data Analysis (EDA): - Dataset for IoT Device 3
"""
## Plot the data:

plt.figure(figsize = (20, 10))
my_circle = plt.Circle((0, 0), 0.7, color='white')
plt.pie(d3['type'].value_counts(), labels=['Gafgyt Udp attack', 'Gafgyt Tcp attack', 'Gafgyt Combo attack', 'Normal Traffic data', 'Gafgyt Junk attack', 'Gafgyt Scan attack'],
        colors=['red', 'green', 'blue', 'orange', 'yellow', 'pink', 'olive', 'darkgrey', 'sienna', 'purple', 'rosybrown'], autopct = '%1.1f%%', textprops={'fontsize': 17, })
p = plt.gcf()
p.gca().add_artist(my_circle)
plt.title('IoT Device 3 Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 26, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('IoT Device 3 Showing the Normal-Benign and Malicious Attacks in a Pie Chart_3B.png')
plt.show()

# Load Dataset for IoT Device 4:
benign = pd.read_csv('/content/nbaiot-dataset/4.benign.csv')
benign.shape

# Botnet --> Mirai
# Attack --> Scan volnerable devices / Ack flooding / Syn flooding / UDP flooding / UDPplain

m_scan = pd.read_csv('/content/nbaiot-dataset/4.mirai.scan.csv')
m_ack = pd.read_csv('/content/nbaiot-dataset/4.mirai.ack.csv')
m_syn = pd.read_csv('/content/nbaiot-dataset/4.mirai.syn.csv')
m_udp = pd.read_csv('/content/nbaiot-dataset/4.mirai.udp.csv')
m_udpplain = pd.read_csv('/content/nbaiot-dataset/4.mirai.udpplain.csv')

#Botnet --> Gafgyt (BASHLITE)
#Attack --> Scan vulnerable devices / Sending spam / UDP flooding / TCP flooding / Sending spam & opening connection
g_scan = pd.read_csv('/content/nbaiot-dataset/4.gafgyt.scan.csv')
g_junk = pd.read_csv('/content/nbaiot-dataset/4.gafgyt.junk.csv')
g_udp = pd.read_csv('/content/nbaiot-dataset/4.gafgyt.udp.csv')
g_tcp = pd.read_csv('/content/nbaiot-dataset/4.gafgyt.tcp.csv')
g_combo = pd.read_csv('/content/nbaiot-dataset/4.gafgyt.combo.csv')

## Convert into one dataset
# add new column
benign['type'] = 'benign'
m_scan['type'] = 'm_scan'
m_ack['type'] = 'm_ack'
m_syn['type']  = 'm_syn'
m_udp['type'] = 'm_udp'
m_udpplain['type'] = 'm_udpplain'
g_combo['type'] = 'g_combo'
g_junk['type'] = 'g_junk'
g_scan['type'] = 'g_scan'
g_tcp['type'] = 'g_tcp'
g_udp['type'] = 'g_udp'

d = pd.concat([benign,m_scan,m_ack,m_syn,m_udp,m_udpplain,g_combo,g_junk,g_scan,g_tcp,g_udp], axis=0, sort=False, ignore_index=True)
d.shape

#value counts
labels = d['type'].value_counts()
label_count = pd.DataFrame({'labels': labels.index,'counts': labels.values})

# Plot the data:
labels = label_count['labels']
sizes = label_count['counts']
plt.figure(figsize=(8,6))
colors = ['red', 'orange', 'yellow', 'green', 'blue', 'lightskyblue', 'pink', 'purple', 'rosybrown', 'olive', 'darkgrey']
# colors = [1,2,3,4,5,6,7,8,9,10,11]
explode = (0, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0)
patches, texts = plt.pie(sizes, explode = explode, colors=colors, shadow=True, startangle=100, pctdistance=100, labeldistance=10)
plt.legend(patches, labels, loc="best")
plt.axis('equal')
plt.tight_layout()
plt.title('IoT Device 4 Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 26, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('IoT Device 4 Showing the Normal-Benign and Malicious Attacks in a Pie Chart_4A.png')
plt.show()

# Save the dataset
d.to_csv('/content/Device #4.csv')

# Load the data
d4 = pd.read_csv('/content/Device #4.csv')

"""
   Visualize the Dataset for IoT Device 4:
"""
print(d4.info())
print(d4.shape)
print(d4.describe())
d4

# Dataset for IoT Device 4 Classes:
d4['type'].value_counts()

"""
   Exploratory Data Analysis (EDA): - Dataset for IoT Device 4
"""
## Plot the data:

plt.figure(figsize = (20, 10))
my_circle = plt.Circle((0, 0), 0.7, color='white')
plt.pie(d4['type'].value_counts(), labels=['Mirai Udp attack', 'Normal Traffic data', 'Mirai Syn attack', 'Gafgyt Udp attack', 'Mirai Scan attack', 'Gafgyt Tcp attack', 'Mirai Ack attack',
                                                   'Mirai udpplain', 'Gafgyt Combo attack', 'Gafgyt Junk attack', 'Gafgyt Scan attack'],
        colors=['red', 'green', 'blue', 'orange', 'yellow', 'pink', 'olive', 'darkgrey', 'sienna', 'purple', 'rosybrown'], autopct = '%1.1f%%', textprops={'fontsize': 17, })
p = plt.gcf()
p.gca().add_artist(my_circle)
plt.title('IoT Device 4 Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 26, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('IoT Device 4 Showing the Normal-Benign and Malicious Attacks in a Pie Chart_4B.png')
plt.show()

# Load Dataset for IoT Device 5:
benign = pd.read_csv('/content/nbaiot-dataset/5.benign.csv')
benign.shape

# Botnet --> Mirai
# Attack --> Scan volnerable devices / Ack flooding / Syn flooding / UDP flooding / UDPplain

m_scan = pd.read_csv('/content/nbaiot-dataset/5.mirai.scan.csv')
m_ack = pd.read_csv('/content/nbaiot-dataset/5.mirai.ack.csv')
m_syn = pd.read_csv('/content/nbaiot-dataset/5.mirai.syn.csv')
m_udp = pd.read_csv('/content/nbaiot-dataset/5.mirai.udp.csv')
m_udpplain = pd.read_csv('/content/nbaiot-dataset/5.mirai.udpplain.csv')

#Botnet --> Gafgyt (BASHLITE)
#Attack --> Scan vulnerable devices / Sending spam / UDP flooding / TCP flooding / Sending spam & opening connection
g_scan = pd.read_csv('/content/nbaiot-dataset/5.gafgyt.scan.csv')
g_junk = pd.read_csv('/content/nbaiot-dataset/5.gafgyt.junk.csv')
g_udp = pd.read_csv('/content/nbaiot-dataset/5.gafgyt.udp.csv')
g_tcp = pd.read_csv('/content/nbaiot-dataset/5.gafgyt.tcp.csv')
g_combo = pd.read_csv('/content/nbaiot-dataset/5.gafgyt.combo.csv')

## Convert into one dataset
# add new column
benign['type'] = 'benign'
m_scan['type'] = 'm_scan'
m_ack['type'] = 'm_ack'
m_syn['type']  = 'm_syn'
m_udp['type'] = 'm_udp'
m_udpplain['type'] = 'm_udpplain'
g_combo['type'] = 'g_combo'
g_junk['type'] = 'g_junk'
g_scan['type'] = 'g_scan'
g_tcp['type'] = 'g_tcp'
g_udp['type'] = 'g_udp'

d = pd.concat([benign,m_scan,m_ack,m_syn,m_udp,m_udpplain,g_combo,g_junk,g_scan,g_tcp,g_udp], axis=0, sort=False, ignore_index=True)
d.shape

#value counts
labels = d['type'].value_counts()
label_count = pd.DataFrame({'labels': labels.index,'counts': labels.values})

# Plot the data:
labels = label_count['labels']
sizes = label_count['counts']
plt.figure(figsize=(8,6))
colors = ['red', 'orange', 'yellow', 'green', 'blue', 'lightskyblue', 'pink', 'purple', 'rosybrown', 'olive', 'darkgrey']
# colors = [1,2,3,4,5,6,7,8,9,10,11]
explode = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1)
patches, texts = plt.pie(sizes, explode = explode, colors=colors, shadow=True, startangle=100, pctdistance=100, labeldistance=10)
plt.legend(patches, labels, loc="best")
plt.axis('equal')
plt.tight_layout()
plt.title('IoT Device 5 Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 26, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('IoT Device 5 Showing the Normal-Benign and Malicious Attacks in a Pie Chart_5A.png')
plt.show()

# Save the dataset
d.to_csv('/content/Device #5.csv')

# Load the data
d5 = pd.read_csv('/content/Device #5.csv')

"""
   Visualize the Dataset for IoT Device 5:
"""
print(d5.info())
print(d5.shape)
print(d5.describe())
d5

# Dataset for IoT Device 5 Classes:
d5['type'].value_counts()

"""
   Exploratory Data Analysis (EDA): - Dataset for IoT Device 5
"""
## Plot the data:

plt.figure(figsize = (20, 10))
my_circle = plt.Circle((0, 0), 0.7, color='white')
plt.pie(d5['type'].value_counts(), labels=['Mirai Udp attack', 'Gafgyt Tcp attack', 'Gafgyt Udp attack', 'Mirai Scan attack', 'Mirai Syn attack', 'Normal Traffic data',
                                                    'Gafgyt Combo attack', 'Mirai Ack attack', 'Mirai udpplain', 'Gafgyt Junk attack', 'Gafgyt Scan attack'],
        colors=['red', 'green', 'blue', 'orange', 'yellow', 'pink', 'olive', 'darkgrey', 'sienna', 'purple', 'rosybrown'], autopct = '%1.1f%%', textprops={'fontsize': 17, })
p = plt.gcf()
p.gca().add_artist(my_circle)
plt.title('IoT Device 5 Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 26, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('IoT Device 5 Showing the Normal-Benign and Malicious Attacks in a Pie Chart_5B.png')
plt.show()

# Load Dataset for IoT Device 6:
benign = pd.read_csv('/content/nbaiot-dataset/6.benign.csv')
benign.shape

# Botnet --> Mirai
# Attack --> Scan volnerable devices / Ack flooding / Syn flooding / UDP flooding / UDPplain

m_scan = pd.read_csv('/content/nbaiot-dataset/6.mirai.scan.csv')
m_ack = pd.read_csv('/content/nbaiot-dataset/6.mirai.ack.csv')
m_syn = pd.read_csv('/content/nbaiot-dataset/6.mirai.syn.csv')
m_udp = pd.read_csv('/content/nbaiot-dataset/6.mirai.udp.csv')
m_udpplain = pd.read_csv('/content/nbaiot-dataset/6.mirai.udpplain.csv')

#Botnet --> Gafgyt (BASHLITE)
#Attack --> Scan vulnerable devices / Sending spam / UDP flooding / TCP flooding / Sending spam & opening connection
g_scan = pd.read_csv('/content/nbaiot-dataset/6.gafgyt.scan.csv')
g_junk = pd.read_csv('/content/nbaiot-dataset/6.gafgyt.junk.csv')
g_udp = pd.read_csv('/content/nbaiot-dataset/6.gafgyt.udp.csv')
g_tcp = pd.read_csv('/content/nbaiot-dataset/6.gafgyt.tcp.csv')
g_combo = pd.read_csv('/content/nbaiot-dataset/6.gafgyt.combo.csv')

##Convert into one dataset
# add new column
benign['type'] = 'benign'
m_scan['type'] = 'm_scan'
m_ack['type'] = 'm_ack'
m_syn['type']  = 'm_syn'
m_udp['type'] = 'm_udp'
m_udpplain['type'] = 'm_udpplain'
g_combo['type'] = 'g_combo'
g_junk['type'] = 'g_junk'
g_scan['type'] = 'g_scan'
g_tcp['type'] = 'g_tcp'
g_udp['type'] = 'g_udp'

d = pd.concat([benign,m_scan,m_ack,m_syn,m_udp,m_udpplain,g_combo,g_junk,g_scan,g_tcp,g_udp], axis=0, sort=False, ignore_index=True)
d.shape

#value counts
labels = d['type'].value_counts()
label_count = pd.DataFrame({'labels': labels.index,'counts': labels.values})

# Plot the data:
labels = label_count['labels']
sizes = label_count['counts']
plt.figure(figsize=(8,6))
colors = ['red', 'orange', 'yellow', 'green', 'blue', 'lightskyblue', 'pink', 'purple', 'rosybrown', 'olive', 'darkgrey']
# colors = [1,2,3,4,5,6,7,8,9,10,11]
explode = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1)
patches, texts = plt.pie(sizes, explode = explode, colors=colors, shadow=True, startangle=100, pctdistance=100, labeldistance=10)
plt.legend(patches, labels, loc="best")
plt.axis('equal')
plt.tight_layout()
plt.title('IoT Device 6 Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 26, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('IoT Device 6 Showing the Normal-Benign and Malicious Attacks in a Pie Chart_6A.png')
plt.show()

# Save the dataset
d.to_csv('/content/Device #6.csv')

# Load the data
d6 = pd.read_csv('/content/Device #6.csv')

"""
   Visualize the Dataset for IoT Device 6:
"""
print(d6.info())
print(d6.shape)
print(d6.describe())
d6

# Dataset for IoT Device 6 Classes:
d6['type'].value_counts()

"""
   Exploratory Data Analysis (EDA): - Dataset for IoT Device 6
"""
## Plot the data:

plt.figure(figsize = (20, 10))
my_circle = plt.Circle((0, 0), 0.7, color='white')
plt.pie(d6['type'].value_counts(), labels=['Mirai Udp attack', 'Gafgyt Udp attack', 'Normal Traffic data', 'Mirai Scan attack', 'Gafgyt Tcp attack', 'Mirai Syn attack', 'Mirai Ack attack',
                                                   'Gafgyt Combo attack', 'Mirai udpplain', 'Gafgyt Junk attack', 'Gafgyt Scan attack'],
        colors=['red', 'green', 'blue', 'orange', 'yellow', 'pink', 'olive', 'darkgrey', 'sienna', 'purple', 'rosybrown'], autopct = '%1.1f%%', textprops={'fontsize': 17, })
p = plt.gcf()
p.gca().add_artist(my_circle)
plt.title('IoT Device 6 Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 26, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('IoT Device 6 Showing the Normal-Benign and Malicious Attacks in a Pie Chart_6B.png')
plt.show()

# Load Dataset for IoT Device 7:
benign = pd.read_csv('/content/nbaiot-dataset/7.benign.csv')
benign.shape

#Botnet --> Gafgyt (BASHLITE)
#Attack --> Scan vulnerable devices / Sending spam / UDP flooding / TCP flooding / Sending spam & opening connection
g_scan = pd.read_csv('/content/nbaiot-dataset/7.gafgyt.scan.csv')
g_junk = pd.read_csv('/content/nbaiot-dataset/7.gafgyt.junk.csv')
g_udp = pd.read_csv('/content/nbaiot-dataset/7.gafgyt.udp.csv')
g_tcp = pd.read_csv('/content/nbaiot-dataset/7.gafgyt.tcp.csv')
g_combo = pd.read_csv('/content/nbaiot-dataset/7.gafgyt.combo.csv')

##Convert into one dataset
# add new column
benign['type'] = 'benign'
g_combo['type'] = 'g_combo'
g_junk['type'] = 'g_junk'
g_scan['type'] = 'g_scan'
g_tcp['type'] = 'g_tcp'
g_udp['type'] = 'g_udp'

d = pd.concat([benign,g_combo,g_junk,g_scan,g_tcp,g_udp], axis=0, sort=False, ignore_index=True)
d.shape

#value counts
labels = d['type'].value_counts()
label_count = pd.DataFrame({'labels': labels.index,'counts': labels.values})

# Plot the Data:
labels = label_count['labels']
sizes = label_count['counts']
plt.figure(figsize=(8,6))
colors = ['red', 'orange', 'yellow', 'green', 'olive', 'darkgrey']
# colors = [1,2,3,4,5,6,7,8,9,10,11]
explode = (0, 0, 0, 0.1, 0, 0)
patches, texts = plt.pie(sizes, explode = explode, colors=colors, shadow=True, startangle=100, pctdistance=100, labeldistance=10)
plt.legend(patches, labels, loc="best")
plt.axis('equal')
plt.tight_layout()
plt.title('IoT Device 7 Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 26, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('IoT Device 7 Showing the Normal-Benign and Malicious Attacks in a Pie Chart_7A.png')
plt.show()

# Save the dataset
d.to_csv('/content/Device #7.csv')

# Load the data
d7 = pd.read_csv('/content/Device #7.csv')

"""
   Visualize the Dataset for IoT Device 7:
"""
print(d7.info())
print(d7.shape)
print(d7.describe())
d7

# Dataset for IoT Device 7 Classes:
d7['type'].value_counts()

"""
   Exploratory Data Analysis (EDA): - Dataset for IoT Device 7
"""
## Plot the data:

plt.figure(figsize = (20, 10))
my_circle = plt.Circle((0, 0), 0.7, color='white')
plt.pie(d3['type'].value_counts(), labels=['Gafgyt Udp attack', 'Gafgyt Tcp attack', 'Gafgyt Combo attack', 'Normal Traffic data', 'Gafgyt Junk attack', 'Gafgyt Scan attack'],
        colors=['pink', 'olive', 'darkgrey', 'sienna', 'purple', 'rosybrown'], autopct = '%1.1f%%', textprops={'fontsize': 17, })
p = plt.gcf()
p.gca().add_artist(my_circle)
plt.title('IoT Device 7 Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 26, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('IoT Device 7 Showing the Normal-Benign and Malicious Attacks in a Pie Chart_7B.png')
plt.show()

# Load Dataset for IoT Device 8:
benign = pd.read_csv('/content/nbaiot-dataset/8.benign.csv')
benign.shape

# Botnet --> Mirai
# Attack --> Scan volnerable devices / Ack flooding / Syn flooding / UDP flooding / UDPplain

m_scan = pd.read_csv('/content/nbaiot-dataset/8.mirai.scan.csv')
m_ack = pd.read_csv('/content/nbaiot-dataset/8.mirai.ack.csv')
m_syn = pd.read_csv('/content/nbaiot-dataset/8.mirai.syn.csv')
m_udp = pd.read_csv('/content/nbaiot-dataset/8.mirai.udp.csv')
m_udpplain = pd.read_csv('/content/nbaiot-dataset/8.mirai.udpplain.csv')

#Botnet --> Gafgyt (BASHLITE)
#Attack --> Scan vulnerable devices / Sending spam / UDP flooding / TCP flooding / Sending spam & opening connection
g_scan = pd.read_csv('/content/nbaiot-dataset/8.gafgyt.scan.csv')
g_junk = pd.read_csv('/content/nbaiot-dataset/8.gafgyt.junk.csv')
g_udp = pd.read_csv('/content/nbaiot-dataset/8.gafgyt.udp.csv')
g_tcp = pd.read_csv('/content/nbaiot-dataset/8.gafgyt.tcp.csv')
g_combo = pd.read_csv('/content/nbaiot-dataset/8.gafgyt.combo.csv')

##Convert into one dataset
# add new column
benign['type'] = 'benign'
m_scan['type'] = 'm_scan'
m_ack['type'] = 'm_ack'
m_syn['type']  = 'm_syn'
m_udp['type'] = 'm_udp'
m_udpplain['type'] = 'm_udpplain'
g_combo['type'] = 'g_combo'
g_junk['type'] = 'g_junk'
g_scan['type'] = 'g_scan'
g_tcp['type'] = 'g_tcp'
g_udp['type'] = 'g_udp'

d = pd.concat([benign,m_scan,m_ack,m_syn,m_udp,m_udpplain,g_combo,g_junk,g_scan,g_tcp,g_udp], axis=0, sort=False, ignore_index=True)
d.shape

#value counts
labels = d['type'].value_counts()
label_count = pd.DataFrame({'labels': labels.index,'counts': labels.values})

# Plot the data:
labels = label_count['labels']
sizes = label_count['counts']
plt.figure(figsize=(8,6))
colors = ['red', 'orange', 'yellow', 'green', 'blue', 'lightskyblue', 'pink', 'purple', 'rosybrown', 'olive', 'darkgrey']
# colors = [1,2,3,4,5,6,7,8,9,10,11]
explode = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1)
patches, texts = plt.pie(sizes, explode = explode, colors=colors, shadow=True, startangle=100, pctdistance=100, labeldistance=10)
plt.legend(patches, labels, loc="best")
plt.axis('equal')
plt.tight_layout()
plt.title('IoT Device 8 Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 26, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('IoT Device 8 Showing the Normal-Benign and Malicious Attacks in a Pie Chart_8A.png')
plt.show()

# Save the dataset
d.to_csv('/content/Device #8.csv')

# Load the data
d8 = pd.read_csv('/content/Device #8.csv')

"""
   Visualize the Dataset for IoT Device 8:
"""
print(d8.info())
print(d8.shape)
print(d8.describe())
d8

# Dataset for IoT Device 8 Classes:
d8['type'].value_counts()

"""
   Exploratory Data Analysis (EDA): - Dataset for IoT Device 8
"""
## Plot the data:

plt.figure(figsize = (20, 10))
my_circle = plt.Circle((0, 0), 0.7, color='white')
plt.pie(d8['type'].value_counts(), labels=['Mirai Udp attack', 'Mirai Syn attack', 'Mirai Ack attack', 'Gafgyt Udp attack', 'Gafgyt Tcp attack',
                                                   'Mirai udpplain', 'Gafgyt Combo attack', 'Normal Traffic data', 'Mirai Scan attack', 'Gafgyt Junk attack', 'Gafgyt Scan attack'],
        colors=['red', 'green', 'blue', 'orange', 'yellow', 'pink', 'olive', 'darkgrey', 'sienna', 'purple', 'rosybrown'], autopct = '%1.1f%%', textprops={'fontsize': 17, })
p = plt.gcf()
p.gca().add_artist(my_circle)
plt.title('IoT Device 8 Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 26, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('IoT Device 8 Showing the Normal-Benign and Malicious Attacks in a Pie Chart_8B.png')
plt.show()

# Load Dataset for IoT Device 9:
benign = pd.read_csv('/content/nbaiot-dataset/9.benign.csv')
benign.shape

# Botnet --> Mirai
# Attack --> Scan volnerable devices / Ack flooding / Syn flooding / UDP flooding / UDPplain

m_scan = pd.read_csv('/content/nbaiot-dataset/9.mirai.scan.csv')
m_ack = pd.read_csv('/content/nbaiot-dataset/9.mirai.ack.csv')
m_syn = pd.read_csv('/content/nbaiot-dataset/9.mirai.syn.csv')
m_udp = pd.read_csv('/content/nbaiot-dataset/9.mirai.udp.csv')
m_udpplain = pd.read_csv('/content/nbaiot-dataset/9.mirai.udpplain.csv')

#Botnet --> Gafgyt (BASHLITE)
#Attack --> Scan vulnerable devices / Sending spam / UDP flooding / TCP flooding / Sending spam & opening connection
g_scan = pd.read_csv('/content/nbaiot-dataset/9.gafgyt.scan.csv')
g_junk = pd.read_csv('/content/nbaiot-dataset/9.gafgyt.junk.csv')
g_udp = pd.read_csv('/content/nbaiot-dataset/9.gafgyt.udp.csv')
g_tcp = pd.read_csv('/content/nbaiot-dataset/9.gafgyt.tcp.csv')
g_combo = pd.read_csv('/content/nbaiot-dataset/9.gafgyt.combo.csv')

##Convert into one dataset
# add new column
benign['type'] = 'benign'
m_scan['type'] = 'm_scan'
m_ack['type'] = 'm_ack'
m_syn['type']  = 'm_syn'
m_udp['type'] = 'm_udp'
m_udpplain['type'] = 'm_udpplain'
g_combo['type'] = 'g_combo'
g_junk['type'] = 'g_junk'
g_scan['type'] = 'g_scan'
g_tcp['type'] = 'g_tcp'
g_udp['type'] = 'g_udp'

d = pd.concat([benign,m_scan,m_ack,m_syn,m_udp,m_udpplain,g_combo,g_junk,g_scan,g_tcp,g_udp], axis=0, sort=False, ignore_index=True)
d.shape

#value counts
labels = d['type'].value_counts()
label_count = pd.DataFrame({'labels': labels.index,'counts': labels.values})

# Plot the data:
labels = label_count['labels']
sizes = label_count['counts']
plt.figure(figsize=(8,6))
colors = ['red', 'orange', 'yellow', 'green', 'blue', 'lightskyblue', 'pink', 'purple', 'rosybrown', 'olive', 'darkgrey']
# colors = [1,2,3,4,5,6,7,8,9,10,11]
explode = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1)
patches, texts = plt.pie(sizes, explode = explode, colors=colors, shadow=True, startangle=100, pctdistance=100, labeldistance=10)
plt.legend(patches, labels, loc="best")
plt.axis('equal')
plt.tight_layout()
plt.title('IoT Device 9 Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 26, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('IoT Device 9 Showing the Normal-Benign and Malicious Attacks in a Pie Chart_9A.png')
plt.show()

# Save the dataset
d.to_csv('/content/Device #9.csv')

# Load the data
d9 = pd.read_csv('/content/Device #9.csv')

"""
   Visualize the Dataset for IoT Device 9:
"""
print(d9.info())
print(d9.shape)
print(d9.describe())
d9

# Dataset for IoT Device 9 Classes:
d9['type'].value_counts()

"""
   Exploratory Data Analysis (EDA): - Dataset for IoT Device 9
"""
## Plot the data:

plt.figure(figsize = (20, 10))
my_circle = plt.Circle((0, 0), 0.7, color='white')
plt.pie(d9['type'].value_counts(), labels=['Mirai Udp attack', 'Mirai Syn attack', 'Mirai Ack attack', 'Gafgyt Udp attack', 'Gafgyt Tcp attack',
                                                   'Mirai udpplain', 'Gafgyt Combo attack', 'Mirai Scan attack', 'Gafgyt Scan attack', 'Gafgyt Junk attack', 'Normal Traffic data'],
        colors=['red', 'green', 'blue', 'orange', 'yellow', 'pink', 'olive', 'darkgrey', 'sienna', 'purple', 'rosybrown'], autopct = '%1.1f%%', textprops={'fontsize': 17, })
p = plt.gcf()
p.gca().add_artist(my_circle)
plt.title('IoT Device 9 Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 26, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('IoT Device 9 Showing the Normal-Benign and Malicious Attacks in a Pie Chart_9B.png')
plt.show()

"""
   Concatenate All 9 Devices Data
"""

All_Device_Data = pd.concat([d1, d2, d3, d4, d5, d6, d7, d8, d9], axis=0, sort=False, ignore_index=True)

"""
   Visualize all the 9 IoT Devices:
"""
print(All_Device_Data.info())
print(All_Device_Data.shape)
print(All_Device_Data.describe())
All_Device_Data

# Dataset for all 9 IoT Devices Classes:
All_Device_Data['type'].value_counts()

"""
   Exploratory Data Analysis (EDA): - All Dataset for the 9 IoT Devices
"""
## Plot the data:

plt.figure(figsize = (20, 10))
my_circle = plt.Circle((0, 0), 0.7, color='white')
plt.pie(All_Device_Data['type'].value_counts(), labels=['Mirai Udp attack', 'Mirai Syn attack', 'Mirai Ack attack', 'Gafgyt Udp attack', 'Gafgyt Tcp attack',
                                                   'Mirai udpplain', 'Gafgyt Combo attack', 'Mirai Scan attack', 'Gafgyt Scan attack', 'Gafgyt Junk attack', 'Normal Traffic data'],
        colors=['red', 'green', 'blue', 'orange', 'yellow', 'pink', 'olive', 'darkgrey', 'sienna', 'purple', 'rosybrown'], autopct = '%1.1f%%', textprops={'fontsize': 17, })
p = plt.gcf()
p.gca().add_artist(my_circle)
plt.title('All 9 IoT Devices Showing the Normal/Benign and Malicious Attacks in a Pie Chart', size = 22, fontweight='bold')
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig('All 9 IoT Devices Showing the Normal-Benign and Malicious Attacks in a Pie Chart_9B.png')
plt.show()

#value counts

labels = All_Device_Data['type'].value_counts()
label_count = pd.DataFrame({'labels': labels.index,'counts': labels.values})

# Plot the Gender Data:

labels = label_count['labels']
sizes = label_count['counts']

total_runs = label_count.counts.sum()

# compute percentage of each format
percentage = []
for i in range(label_count.shape[0]):
    pct = (label_count.counts[i] / total_runs) * 100
    percentage.append(round(pct,2))
label_count['Percentage'] = percentage

label_count

# side-by-side bar plots

plt.rcdefaults()
today = date.today()
width = 0.5
plt.clf()
plt.figure(figsize = (15, 10), facecolor=(1, 1, 1))
plt.style.use('ggplot')
colors = ["mistyrose", "beige", "darkslategray", "slateblue", "rosybrown", "wheat", "slateblue", "lavender", "crimson", "firebrick", "darkcyan"]
graph = plt.barh(labels, sizes, width, edgecolor='white', color = colors)
plt.xlabel("Counts", weight='bold').set_fontsize('22')
plt.ylabel("Benign/Abnormal Attacks", weight='bold').set_fontsize('24')
plt.xticks(rotation=45, size = 16, fontweight='bold')
plt.title("All 9 IoT Devices Showing the Normal/Benign and Malicious Attacks", size = 20, fontweight='bold')


i = 0
for p in graph:
    width = p.get_width()
    height = p.get_height()
    x, y = p.get_xy()
    plt.text(x+width/2,
             y+height*1.01,
             str(label_count.Percentage[i])+'%',
             ha='center',
             weight='bold')
    i+=1

_ = plt.legend(loc = 'best', bbox_to_anchor = (1.0, 1.0), ncol = 1, frameon=True, fontsize = '10')
plt.grid(True)
plt.annotate(f"@yourname {today}.",
             xy=(1, 1),  xycoords='data',
             xytext=(0, -0.20), textcoords='axes fraction',
             color='black')
plt.savefig("All 9 IoT Devices Showing the Normal_Benign and Malicious Attacks.png")

"""
   B. Data Pre-processing:
"""
def pre_processing(data):
    data = data.drop(data.columns[0], axis=1) # Drop the first column

    benign = data[data['type'] == 'benign'] # take benign data only
    benign = benign.drop('type', 1) # drop column 'type'
    benign['type'] = 0 # bring back the column 'type' and equal it to 0

    abnormal = data[data['type'] != 'benign'] # take those data that are not benign
    abnormal = abnormal.drop('type', 1) # drop column 'type'
    abnormal['type'] = 1 # bring back the column 'type' and equal it to 1

    # split benign data evenly into three parts - 33.33% each for train, tr, and test
    benign_train, benign_tr, benign_test = np.split(benign, [int((1/3)*len(benign)), int((2/3)*len(benign))])
    benign_test_mix = benign_test.copy()    # create a copy of the benign_test data

    train_label = benign_train['type'] # obtain the train label - 0

    benign_train = benign_train.drop('type', axis=1) # drop the column 'type'
    benign_tr = benign_tr.drop('type', axis=1) # drop the column 'type'

    # Create Mix data
    abnormal_sample = abnormal.sample(frac = 1) # return a random sample of abnormal data
    mix_temp = pd.concat([benign_test_mix, abnormal_sample]) # concatenate benign_test_mix and abnormal_sample data
    mix_temp = shuffle(mix_temp, random_state = 1)  # shuffle the mixed data

    mix = mix_temp.copy() # create a copy of the mixed data
    mix_data = mix_temp.drop('type', axis=1) # drop the column 'type'
    mix_label = mix['type'] # column 'type' is taken as the label/target

    # Standardization - Scale the data between 0 and 1
    scaler = StandardScaler()
    benign_train = scaler.fit_transform(benign_train) # scale the benign train data
    benign_tr = scaler.transform(benign_tr) # scale the benign data for calculating the threshold
    mix_data = scaler.transform(mix_data) # scale the benign and abnormal data concatenated for evaluation

    # Convert to tensor and load the benign_tr, mix_data, and mix_label data to device and
    # keep the benign_train as original for later use
    benign_tr = torch.from_numpy(np.array(benign_tr)).float().to(device)
    mix_data = torch.from_numpy(np.array(mix_data)).float().to(device)
    mix_label = torch.tensor(np.array(mix_label))
    train_label = torch.tensor(np.array(train_label))

    return benign_train, benign_tr, mix_data, mix_label, train_label

"""
    B. Data Pre-processing - Generate data to test the device for Federated Modeling:
"""
def other_pre_processing(data):
    data = data.drop(data.columns[0], axis=1)

    benign = data[data['type'] == 'benign']
    benign = benign.drop('type', 1)
    benign['type'] = 0

    abnormal = data[data['type'] != 'benign']
    abnormal = abnormal.drop('type', 1)
    abnormal['type'] = 1

    # split benign data to two parts - this is the main difference between the two Pre-processing method.
    benign_tr, benign_test = train_test_split(benign, test_size = 0.2, random_state = 5703)
    benign_tr = benign_tr.drop('type', axis=1)

    # Create Mix data
    abnormal_sample = abnormal.sample(frac = 1)
    mix_temp = pd.concat([benign_test, abnormal_sample])
    mix_temp = shuffle(mix_temp, random_state=1)

    mix = mix_temp.copy()
    mix_data = mix_temp.drop('type', axis=1)
    mix_label = mix['type']

    # Standardisation
    scaler = StandardScaler()
    benign_tr = scaler.fit_transform(benign_tr)
    mix_data = scaler.transform(mix_data)

    benign_tr = torch.from_numpy(np.array(benign_tr)).float().to(device)
    mix_data = torch.from_numpy(np.array(mix_data)).float().to(device)
    mix_label = torch.tensor(np.array(mix_label))

    return benign_tr, mix_data, mix_label

"""
   Pre-process the data for Federated Model training
"""
benign_train_1, benign_tr_1, mix_data_1, mix_label_1, train_label_1 = pre_processing(d1)
benign_train_2, benign_tr_2, mix_data_2, mix_label_2, train_label_2 = pre_processing(d2)
benign_train_3, benign_tr_3, mix_data_3, mix_label_3, train_label_3 = pre_processing(d3)
benign_train_4, benign_tr_4, mix_data_4, mix_label_4, train_label_4 = pre_processing(d4)
benign_train_5, benign_tr_5, mix_data_5, mix_label_5, train_label_5 = pre_processing(d5)
benign_train_6, benign_tr_6, mix_data_6, mix_label_6, train_label_6 = pre_processing(d6)
benign_train_7, benign_tr_7, mix_data_7, mix_label_7, train_label_7 = pre_processing(d7)
benign_train_8, benign_tr_8, mix_data_8, mix_label_8, train_label_8 = pre_processing(d8)
benign_train_9, benign_tr_9, mix_data_9, mix_label_9, train_label_9 = pre_processing(d9)

"""
   Pre-process the data for Non-Federated Model training
"""
#benign_train_all, benign_tr_all, mix_data_all, mix_label_all, train_label_all = pre_processing(All_Device_Data)

"""
    Print the shape of the data:
"""
x = ''
print(f'The Benign training data for device 1 is:', benign_train_1.shape)
print(f'The Benign training label data for device 1 is:', train_label_1.shape)
print(f'The benign training data for device 1 for calcluting Threshold is:', benign_tr_1.shape)
print(f'The Mixed data for device 1 for Model Evaluation is:', mix_data_1.shape)
print(f'The Mixed label data for device 1 for Model Evaluation is:', mix_label_1.shape)

print(x)

print(f'The Benign training data for device 2 is:', benign_train_2.shape)
print(f'The Benign training label data for device 2 is:', train_label_2.shape)
print(f'The benign training data for device 2 for calcluting Threshold is:', benign_tr_2.shape)
print(f'The Mixed data for device 2 for Model Evaluation is:', mix_data_2.shape)
print(f'The Mixed label data for device 2 for Model Evaluation is:', mix_label_2.shape)

print(x)

print(f'The Benign training data for device 3 is:', benign_train_3.shape)
print(f'The Benign training label data for device 3 is:', train_label_3.shape)
print(f'The benign training data for device 3 for calcluting Threshold is:', benign_tr_3.shape)
print(f'The Mixed data for for device 3 Model Evaluation is:', mix_data_3.shape)
print(f'The Mixed label data for device 3 for Model Evaluation is:', mix_label_3.shape)

print(x)

print(f'The Benign training data for device 4 is:', benign_train_4.shape)
print(f'The Benign training label data for device 4 is:', train_label_4.shape)
print(f'The benign training data for device 4 for calcluting Threshold is:', benign_tr_4.shape)
print(f'The Mixed data for for device 4 Model Evaluation is:', mix_data_4.shape)
print(f'The Mixed label data for device 4 for Model Evaluation is:', mix_label_4.shape)

print(x)

print(f'The Benign training data for device 5 is:', benign_train_5.shape)
print(f'The Benign training label data for device 5 is:', train_label_5.shape)
print(f'The benign training data for device 5 for calcluting Threshold is:', benign_tr_5.shape)
print(f'The Mixed data for for device 5 Model Evaluation is:', mix_data_5.shape)
print(f'The Mixed label data for device 5 for Model Evaluation is:', mix_label_5.shape)

print(x)

print(f'The Benign training data for device 6 is:', benign_train_6.shape)
print(f'The Benign training label data for device 6 is:', train_label_6.shape)
print(f'The benign training data for device 6 for calcluting Threshold is:', benign_tr_6.shape)
print(f'The Mixed data for for device 6 Model Evaluation is:', mix_data_6.shape)
print(f'The Mixed label data for device 6 for Model Evaluation is:', mix_label_6.shape)

print(x)

print(f'The Benign training data for device 7 is:', benign_train_7.shape)
print(f'The Benign training label data for device 7 is:', train_label_7.shape)
print(f'The benign training data for device 7 for calcluting Threshold is:', benign_tr_7.shape)
print(f'The Mixed data for device 7 for Model Evaluation is:', mix_data_7.shape)
print(f'The Mixed label data for device 7 for Model Evaluation is:', mix_label_7.shape)

print(x)

print(f'The Benign training data for device 8 is:', benign_train_8.shape)
print(f'The Benign training label data for device 8 is:', train_label_8.shape)
print(f'The benign training data for device 8 for calcluting Threshold is:', benign_tr_8.shape)
print(f'The Mixed data for for device 8 Model Evaluation is:', mix_data_8.shape)
print(f'The Mixed label data for device 8 for Model Evaluation is:', mix_label_8.shape)

print(x)

print(f'The Benign training data for device 9 is:', benign_train_9.shape)
print(f'The Benign training label data for device 9 is:', train_label_9.shape)
print(f'The benign training data for device 9 for calcluting Threshold is:', benign_tr_9.shape)
print(f'The Mixed data for for device 9 Model Evaluation is:', mix_data_9.shape)
print(f'The Mixed label data for device 9 for Model Evaluation is:', mix_label_9.shape)

"""
   Data Augmentation - Benign Traning Data for Device 1

   Since our data in biased, we need to use data argumentation on it so that we can remove bias from data and make equal distributions.
"""

#print(benign_train_1.info())
#print(benign_train_1.shape)
#print(benign_train_1.describe())
#benign_train_1

# Dataset for IoT Device 1 Classes:
#benign_train_1['type'].value_counts()

"""
    Build the DL Models - Deep Auto-Encoder (AE):
"""
## Deep Auto-Encoder model

input_dim = 115
class AEModel(nn.Module):
    def __init__(self):
        super(AEModel,self).__init__()

        self.encoder = nn.Sequential(
            nn.Linear(input_dim, int(0.75*input_dim)),
            nn.Tanh(),
            nn.Linear(int(0.75*input_dim), int(0.5*input_dim)),
            nn.Tanh(),
            nn.Linear(int(0.5*input_dim), int(0.33*input_dim)),
            nn.Tanh(),
            nn.Linear(int(0.33*input_dim), int(0.25*input_dim)),
            nn.Tanh(),
        )

        self.decoder = nn.Sequential(
            nn.Linear(int(0.25*input_dim), int(0.33*input_dim)),
            nn.Tanh(),
            nn.Linear(int(0.33*input_dim), int(0.5*input_dim)),
            nn.Tanh(),
            nn.Linear(int(0.5*input_dim), int(0.75*input_dim)),
            nn.Tanh(),
            nn.Linear(int(0.75*input_dim), int(input_dim)),
            nn.Tanh(),
        )
    def forward(self, x):
        encode = self.encoder(x)
        decoder = self.decoder(encode)
        return decoder

from typing_extensions import Self
"""
    Build the DL Models - CNN MODEL:
"""

## Define the Sigmoid Linear Unit function also known as the Swish function, instead of using the ReLU function (Novelty)
class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)

## Define the Convolutional Skip-Connection Module
class ConvNormPool(nn.Module):
    def __init__(
        self,
        input_size,
        hidden_size,
        kernel_size,
        norm_type='bachnorm'
    ):
        super(ConvNormPool, self).__init__()

        self.kernel_size = kernel_size
        self.conv_1 = nn.Conv1d(
            in_channels=input_size,
            out_channels=hidden_size,
            kernel_size=kernel_size
        )
        self.conv_2 = nn.Conv1d(
            in_channels=hidden_size,
            out_channels=hidden_size,
            kernel_size=kernel_size
        )
        self.conv_3 = nn.Conv1d(
            in_channels=hidden_size,
            out_channels=hidden_size,
            kernel_size=kernel_size
        )
        self.swish_1 = Swish()
        self.swish_2 = Swish()
        self.swish_3 = Swish()
        if norm_type == 'group':
            self.normalization_1 = nn.GroupNorm(
                num_groups=8,
                num_channels=hidden_size
            )
            self.normalization_2 = nn.GroupNorm(
                num_groups=8,
                num_channels=hidden_size
            )
            self.normalization_3 = nn.GroupNorm(
                num_groups=8,
                num_channels=hidden_size
            )
        else:
            self.normalization_1 = nn.BatchNorm1d(num_features=hidden_size)
            self.normalization_2 = nn.BatchNorm1d(num_features=hidden_size)
            self.normalization_3 = nn.BatchNorm1d(num_features=hidden_size)

        self.pool = nn.MaxPool1d(kernel_size=2)

    def forward(self, input):
        conv1 = self.conv_1(input)
        x = self.normalization_1(conv1)
        x = self.swish_1(x)
        x = F.pad(x, pad=(self.kernel_size - 1, 0))

        x = self.conv_2(x)
        x = self.normalization_2(x)
        x = self.swish_2(x)
        x = F.pad(x, pad=(self.kernel_size - 1, 0))

        conv3 = self.conv_3(x)
        x = self.normalization_3(conv1+conv3)
        x = self.swish_3(x)
        x = F.pad(x, pad=(self.kernel_size - 1, 0))

        x = self.pool(x)
        return x

## Define the CNN Model
class CNN(nn.Module):
    def __init__(
        self,
        input_size = 115, # number of features
        hid_size = 256,
        kernel_size = 11, # number of classes (e.g for device 1)
        num_classes = 11, # number of classes
    ):

        super(CNN, Self).__init__()

        self.conv1 = ConvNormPool(
            input_size=input_size,
            hidden_size=hid_size,
            kernel_size=kernel_size,
        )
        self.conv2 = ConvNormPool(
            input_size=hid_size,
            hidden_size=hid_size//2,
            kernel_size=kernel_size,
        )
        self.conv3 = ConvNormPool(
            input_size=hid_size//2,
            hidden_size=hid_size//4,
            kernel_size=kernel_size,
        )
        self.avgpool = nn.AdaptiveAvgPool1d((1))
        self.fc = nn.Linear(in_features = hid_size//4, out_features = num_classes)

    def forward(self, input):
        x = self.conv1(input)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.avgpool(x)
        # print(x.shape) # num_features * num_channels
        x = x.view(-1, x.size(1) * x.size(2))
        x = F.softmax(self.fc(x), dim=1)
        return x

"""
    Build the DL Models - CNN-LSTM MODEL:
"""

## Define the LSTM Recurrent Neural Network Module (3 Layers):
class RNN(nn.Module):
    def __init__(
        self,
        input_size,
        hid_size,
        num_rnn_layers = 3,
        dropout_p = 0.2,
        bidirectional = False,
        rnn_type = 'lstm',
    ):
        super().__init__()

        self.rnn_layer = nn.LSTM(
            input_size = input_size,
            hidden_size = hid_size,
            num_layers = num_rnn_layers,
            dropout = dropout_p if num_rnn_layers > 1 else 0,
            bidirectional = bidirectional,
            batch_first = True,
        )

    def forward(self, input):
        outputs, hidden_states = self.rnn_layer(input)
        return outputs, hidden_states

## Define the CNN-LSTM Model:
class RNNModel(nn.Module):
    def __init__(
        self,
        input_size,
        hid_size,
        rnn_type,
        bidirectional,
        n_classes = 11,
        kernel_size = 11,
    ):
        super().__init__()

        self.rnn_layer = RNN(
            input_size = 115,
            hid_size = hid_size,
            rnn_type = rnn_type,
            bidirectional = bidirectional
        )
        self.conv1 = ConvNormPool(
            input_size = input_size,
            hidden_size = hid_size,
            kernel_size = kernel_size,
        )
        self.conv2 = ConvNormPool(
            input_size = hid_size,
            hidden_size = hid_size,
            kernel_size = kernel_size,
        )
        self.avgpool = nn.AdaptiveAvgPool1d((1))
        self.fc = nn.Linear(in_features = hid_size, out_features = n_classes)

    def forward(self, input):
        x = self.conv1(input)
        x = self.conv2(x)
        x, _ = self.rnn_layer(x)
        x = self.avgpool(x)
        x = x.view(-1, x.size(1) * x.size(2))
        x = F.softmax(self.fc(x), dim=1) # squeeze(1)
        return x

"""
    Build the DL Models - Attention Based CNN-LSTM:
"""

## Define the Attention-Based CNN-LSTM Model:
class RNNAttentionModel(nn.Module):
    def __init__(
        self,
        input_size,
        hid_size,
        rnn_type,
        bidirectional,
        n_classes = 11,
        kernel_size = 11,
    ):
        super().__init__()

        self.rnn_layer = RNN(
            input_size = 115,
            hid_size = hid_size,
            rnn_type = rnn_type,
            bidirectional = bidirectional
        )
        self.conv1 = ConvNormPool(
            input_size = input_size,
            hidden_size = hid_size,
            kernel_size = kernel_size,
        )
        self.conv2 = ConvNormPool(
            input_size = hid_size,
            hidden_size = hid_size,
            kernel_size = kernel_size,
        )
        self.avgpool = nn.AdaptiveMaxPool1d((1))
        self.attn = nn.Linear(hid_size, hid_size, bias = False)
        self.fc = nn.Linear(in_features = hid_size, out_features = n_classes)

    def forward(self, input):
        x = self.conv1(input)
        x = self.conv2(x)
        x_out, hid_states = self.rnn_layer(x)
        x = torch.cat([hid_states[0], hid_states[1]], dim=0).transpose(0, 1)
        x_attn = torch.tanh(self.attn(x))
        x = x_attn.bmm(x_out)
        x = x.transpose(2, 1)
        x = self.avgpool(x)
        x = x.view(-1, x.size(1) * x.size(2))
        x = F.softmax(self.fc(x), dim=-1)
        return x

"""
    Build the DL Models - CNN-BiLSTM MODEL:
"""

## Define the CNN-BiLSTM Model:
## Define the BiLSTM Recurrent Neural Network Module (3 Layers), and the the CNN-BiLSTM Model:
class RNN(nn.Module):
    def __init__(
        self,
        input_size,
        hid_size,
        num_rnn_layers = 3,
        dropout_p = 0.2,
        bidirectional = True,
        rnn_type = 'lstm',
    ):
        super().__init__()

        self.rnn_layer = nn.LSTM(
            input_size = input_size,
            hidden_size = hid_size,
            num_layers = num_rnn_layers,
            dropout = dropout_p if num_rnn_layers > 1 else 0,
            bidirectional = bidirectional,
            batch_first = True,
        )

    def forward(self, input):
        outputs, hidden_states = self.rnn_layer(input)
        return outputs, hidden_states

class RNNModel(nn.Module):
    def __init__(
        self,
        input_size,
        hid_size,
        rnn_type,
        bidirectional,
        n_classes = 11,
        kernel_size = 11,
    ):
        super().__init__()

        self.rnn_layer = RNN(
            input_size = 115,
            hid_size = hid_size,
            rnn_type = rnn_type,
            bidirectional = bidirectional
        )
        self.conv1 = ConvNormPool(
            input_size = input_size,
            hidden_size = hid_size,
            kernel_size = kernel_size,
        )
        self.conv2 = ConvNormPool(
            input_size = hid_size,
            hidden_size = hid_size,
            kernel_size = kernel_size,
        )
        self.avgpool = nn.AdaptiveAvgPool1d((1))
        self.fc = nn.Linear(in_features = hid_size, out_features = n_classes)

    def forward(self, input):
        x = self.conv1(input)
        x = self.conv2(x)
        x, _ = self.rnn_layer(x)
        x = self.avgpool(x)
        x = x.view(-1, x.size(1) * x.size(2))
        x = F.softmax(self.fc(x), dim=1) # squeeze(1)
        return x

"""
    Build the DL Models - Attention Based CNN-BiLSTM Model:
"""

## Define the Attention-Based CNN-BiLSTM Model:
class RNNAttentionModel(nn.Module):
    def __init__(
        self,
        input_size,
        hid_size,
        rnn_type,
        bidirectional,
        n_classes = 11,
        kernel_size = 11,
    ):
        super().__init__()

        self.rnn_layer = RNN(
            input_size = 115,
            hid_size = hid_size,
            rnn_type = rnn_type,
            bidirectional = bidirectional
        )
        self.conv1 = ConvNormPool(
            input_size = input_size,
            hidden_size = hid_size,
            kernel_size = kernel_size,
        )
        self.conv2 = ConvNormPool(
            input_size = hid_size,
            hidden_size = hid_size,
            kernel_size = kernel_size,
        )
        self.avgpool = nn.AdaptiveMaxPool1d((1))
        self.attn = nn.Linear(hid_size, hid_size, bias = False)
        self.fc = nn.Linear(in_features = hid_size, out_features = n_classes)

    def forward(self, input):
        x = self.conv1(input)
        x = self.conv2(x)
        x_out, hid_states = self.rnn_layer(x)
        x = torch.cat([hid_states[0], hid_states[1]], dim=0).transpose(0, 1)
        x_attn = torch.tanh(self.attn(x))
        x = x_attn.bmm(x_out)
        x = x.transpose(2, 1)
        x = self.avgpool(x)
        x = x.view(-1, x.size(1) * x.size(2))
        x = F.softmax(self.fc(x), dim=-1)
        return x

"""
    Perform Hyper-parameter Optimization using the Randomized Search Algorithm - For Auto Encoder Model and Device 1:

"""
classifier = NeuralNetClassifier(
    AEModel, # for example
    max_epochs = 2,
    lr = 0.1,
    iterator_train__shuffle = True
)
classifier.set_params(train_split=False, verbose=0)

parameters = {
    'lr': [0.0001, 0.001, 0.01],
    'batch_size': [64, 128, 256],
    'max_epochs': [10, 20, 40],
    'optimizer': [torch.optim.Adam, torch.optim.SGD],
}

# Data
X = benign_train_1
y = train_label_1
X = X.astype(np.float32)

start_time=time.time()

gs = RandomizedSearchCV(classifier, parameters, refit=False, cv=3, scoring='accuracy')

gs.fit(X, y)

execution_time = time.time()-start_time

print(gs.best_score_, gs.best_params_)
print(execution_time)

"""
    Perform Hyper-parameter Optimization using the Randomized Search Algorithm - For Auto Encoder Model and Device 2:

"""
classifier = NeuralNetClassifier(
    AEModel, # for example
    max_epochs = 2,
    lr = 0.1,
    iterator_train__shuffle = True
)
classifier.set_params(train_split=False, verbose=0)

parameters = {
    'lr': [0.0001, 0.001, 0.01],
    'batch_size': [64, 128, 256],
    'max_epochs': [10, 20, 40],
    'optimizer': [torch.optim.Adam, torch.optim.SGD],
}

# Data
X = benign_train_2
y = train_label_2
X = X.astype(np.float32)

start_time=time.time()

gs = RandomizedSearchCV(classifier, parameters, refit=False, cv=3, scoring='accuracy')

gs.fit(X, y)

execution_time = time.time()-start_time

print(gs.best_score_, gs.best_params_)
print(execution_time)

"""
    Perform Hyper-parameter Optimization using the Randomized Search Algorithm - For Auto Encoder Model and Device 3:

"""
classifier = NeuralNetClassifier(
    AEModel, # for example
    max_epochs = 2,
    lr = 0.1,
    iterator_train__shuffle = True
)
classifier.set_params(train_split=False, verbose=0)

parameters = {
    'lr': [0.0001, 0.001, 0.01],
    'batch_size': [64, 128, 256],
    'max_epochs': [10, 20, 40],
    'optimizer': [torch.optim.Adam, torch.optim.SGD],
}

# Data
X = benign_train_3
y = train_label_3
X = X.astype(np.float32)

start_time=time.time()

gs = RandomizedSearchCV(classifier, parameters, refit=False, cv=3, scoring='accuracy')

gs.fit(X, y)

execution_time = time.time()-start_time

print(gs.best_score_, gs.best_params_)
print(execution_time)

"""
    Perform Hyper-parameter Optimization using the Randomized Search Algorithm - For Auto Encoder Model and Device 4:

"""
classifier = NeuralNetClassifier(
    AEModel, # for example
    max_epochs = 2,
    lr = 0.1,
    iterator_train__shuffle = True
)
classifier.set_params(train_split=False, verbose=0)

parameters = {
    'lr': [0.0001, 0.001, 0.01],
    'batch_size': [64, 128, 256],
    'max_epochs': [10, 20, 40],
    'optimizer': [torch.optim.Adam, torch.optim.SGD],
}

# Data
X = benign_train_4
y = train_label_4
X = X.astype(np.float32)

start_time=time.time()

gs = RandomizedSearchCV(classifier, parameters, refit=False, cv=3, scoring='accuracy')

gs.fit(X, y)

execution_time = time.time()-start_time

print(gs.best_score_, gs.best_params_)
print(execution_time)

"""
    Perform Hyper-parameter Optimization using the Randomized Search Algorithm - For Auto Encoder Model and Device 5:

"""
classifier = NeuralNetClassifier(
    AEModel, # for example
    max_epochs = 2,
    lr = 0.1,
    iterator_train__shuffle = True
)
classifier.set_params(train_split=False, verbose=0)

parameters = {
    'lr': [0.0001, 0.001, 0.01],
    'batch_size': [64, 128, 256],
    'max_epochs': [10, 20, 40],
    'optimizer': [torch.optim.Adam, torch.optim.SGD],
}

# Data
X = benign_train_5
y = train_label_5
X = X.astype(np.float32)

start_time=time.time()

gs = RandomizedSearchCV(classifier, parameters, refit=False, cv=3, scoring='accuracy')

gs.fit(X, y)

execution_time = time.time()-start_time

print(gs.best_score_, gs.best_params_)
print(execution_time)

"""
    Perform Hyper-parameter Optimization using the Randomized Search Algorithm - For Auto Encoder Model and Device 6:

"""
classifier = NeuralNetClassifier(
    AEModel, # for example
    max_epochs = 2,
    lr = 0.1,
    iterator_train__shuffle = True
)
classifier.set_params(train_split=False, verbose=0)

parameters = {
    'lr': [0.0001, 0.001, 0.01],
    'batch_size': [64, 128, 256],
    'max_epochs': [10, 20, 40],
    'optimizer': [torch.optim.Adam, torch.optim.SGD],
}

# Data
X = benign_train_6
y = train_label_6
X = X.astype(np.float32)

start_time=time.time()

gs = RandomizedSearchCV(classifier, parameters, refit=False, cv=3, scoring='accuracy')

gs.fit(X, y)

execution_time = time.time()-start_time

print(gs.best_score_, gs.best_params_)
print(execution_time)

"""
    Perform Hyper-parameter Optimization using the Randomized Search Algorithm - For Auto Encoder Model and Device 7:

"""
classifier = NeuralNetClassifier(
    AEModel, # for example
    max_epochs = 2,
    lr = 0.1,
    iterator_train__shuffle = True
)
classifier.set_params(train_split=False, verbose=0)

parameters = {
    'lr': [0.0001, 0.001, 0.01],
    'batch_size': [64, 128, 256],
    'max_epochs': [10, 20, 40],
    'optimizer': [torch.optim.Adam, torch.optim.SGD],
}

# Data
X = benign_train_7
y = train_label_7
X = X.astype(np.float32)

start_time=time.time()

gs = RandomizedSearchCV(classifier, parameters, refit=False, cv=3, scoring='accuracy')

gs.fit(X, y)

execution_time = time.time()-start_time

print(gs.best_score_, gs.best_params_)
print(execution_time)

"""
    Perform Hyper-parameter Optimization using the Randomized Search Algorithm - For Auto Encoder Model and Device 8:

"""
classifier = NeuralNetClassifier(
    AEModel, # for example
    max_epochs = 2,
    lr = 0.1,
    iterator_train__shuffle = True
)
classifier.set_params(train_split=False, verbose=0)

parameters = {
    'lr': [0.0001, 0.001, 0.01],
    'batch_size': [64, 128, 256],
    'max_epochs': [10, 20, 40],
    'optimizer': [torch.optim.Adam, torch.optim.SGD],
}

# Data
X = benign_train_8
y = train_label_8
X = X.astype(np.float32)

start_time=time.time()

gs = RandomizedSearchCV(classifier, parameters, refit=False, cv=3, scoring='accuracy')

gs.fit(X, y)

execution_time = time.time()-start_time

print(gs.best_score_, gs.best_params_)
print(execution_time)

"""
    Perform Hyper-parameter Optimization using the Randomized Search Algorithm - For Auto Encoder Model and Device 9:

"""
classifier = NeuralNetClassifier(
    AEModel, # for example
    max_epochs = 2,
    lr = 0.1,
    iterator_train__shuffle = True
)
classifier.set_params(train_split=False, verbose=0)

parameters = {
    'lr': [0.0001, 0.001, 0.01],
    'batch_size': [64, 128, 256],
    'max_epochs': [10, 20, 40],
    'optimizer': [torch.optim.Adam, torch.optim.SGD],
}

# Data
X = benign_train_9
y = train_label_9
X = X.astype(np.float32)

start_time=time.time()

gs = RandomizedSearchCV(classifier, parameters, refit=False, cv=3, scoring='accuracy')

gs.fit(X, y)

execution_time = time.time()-start_time

print(gs.best_score_, gs.best_params_)
print(execution_time)

"""
   The Federated Learning Algorithm - FedAvgM
   Here, we aggregate the model's weights received from every client and updates the global model with updated weights
"""
def server_aggregate_M(global_model, client_models, client_lens):
    total = sum(client_lens)
    n = len(client_models)
    global_dict = global_model.state_dict()
    temp = copy.deepcopy(global_dict)
    v = {x:1 for x in copy.deepcopy(global_dict)}

     # Calculate The Average Weight/Bias --> avg_w/b
    for i, k in enumerate(global_dict.keys()):
        temp[k] = torch.stack([client_models[i].state_dict()[k].float() * (n * client_lens[i] / total) for i in range(len(client_models))], 0).mean(0)
        temp_v = 0.9 * v[k] + temp[k]
        global_dict[k] = global_dict[k] - temp_v
    global_model.load_state_dict(global_dict)

     # update the Local models with the Weight/Bias
    for model in client_models:
        model.load_state_dict(global_model.state_dict())

"""
    Update and Train Client Model on Client Data - Selected Clients are trained locally
"""
def client_update(client_model, optimizer, train_data, epoch = 3):
    model.train()
    for e in range(epoch):
        running_loss = 0.0
        for bx, (data) in enumerate(train_data):
            output = client_model(data.float())
            optimizer.zero_grad()
            loss = nn.MSELoss(reduction='mean')(data.float().to(device), output)
            loss.backward()
            optimizer.step()
            # print(loss.item())
            running_loss += loss.item()
        # print(running_loss)
        epoch_loss = running_loss/len(train_data)
    return epoch_loss

"""
    Before Training Begins, Sychronize the Client Model with Global Weights:
"""
def client_syn(client_model, global_model):
    client_model.load_state_dict(global_model.state_dict())

"""
    Define Evaluation Metrics - Accuracy, Precision, Recall, F1-Score, True Positive Rate (TPR), and False Positive Rate (FPR):
"""
def get_tr(model, tr_data):
    model.eval()
    mse = np.mean(np.power(tr_data.cpu().detach().numpy() - model(tr_data).cpu().detach().numpy(), 2), axis = 1)
    tr = mse.mean() + mse.std()
    return tr

def perf_measure(y_actual, y_pred):
    TP = 0
    FP = 0
    TN = 0
    FN = 0
    for i in range(len(y_pred)):
        if y_actual[i]==y_pred[i]==1:
           TP += 1
        if y_pred[i]==1 and y_actual[i]!=y_pred[i]:
           FP += 1
        if y_actual[i]==y_pred[i]==0:
           TN += 1
        if y_pred[i]==0 and y_actual[i]!=y_pred[i]:
           FN += 1
    return (TP, FP, TN, FN)

def get_mix_result(model, tr, mix_data, mix_label):
    model.eval()
    mse = np.mean(np.power(mix_data.cpu().detach().numpy() - model(mix_data).cpu().detach().numpy(), 2), axis = 1)

    prediction = []
    for i in mse:
        if i > tr:
            prediction.append(1)
        else:
            prediction.append(0)

    mix_label_list = mix_label.tolist()
    TP,FP,TN,FN = perf_measure(mix_label_list, prediction)
    conf = [[TP, FN],[FP, TN]]
    x_axis_label = ['abnormal', 'benign']
    y_axis_label = ['abnormal', 'benign']
    plt.figure()
    sns.heatmap(conf, xticklabels = x_axis_label, yticklabels = y_axis_label, annot = True, annot_kws = {"size": 16}, fmt = 'g')

    acc = (TP+TN) / (TP+TN+FP+FN)
    precision = TP/(TP+FP)
    recall = TP/(TP+FN)
    F1score = 2 * ((precision * recall) / (precision + recall))

    TPR = round((TP / (TP+FN)), 6)
    # print('TPR is: {}%'.format(TPR))

    FPR = round((FP / (FP + TN)), 6)
    # print('TPR is: {}%'.format(FPR))

    print('Acc: %.3f%% \nPrecision: %.3f \nRecall: %.3f \nF1score: %.3f \nTPR: %.5f \nFPR: %.5f'%(acc,
                                                                                                   precision,
                                                                                                   recall,
                                                                                                   F1score,
                                                                                                   TPR,
                                                                                                   FPR))

"""
    Create the Data Generator for Retraining Process by Picking Baseline Data for each Device,
    Create the Data Generator for Retraining Process by Picking Baseline Data for each Device,
    Define Other Training Parameters, Initialize the Model, and Data, Initialize the Optimizer, Data, Devices,
    Train the Federated Model, Load Trained Model, and Test on Client's Devices:

    Using Device 1 Optimized Parameters for the AE Model -  Federated Learning:
    {'optimizer': <class 'torch.optim.sgd.SGD'>, 'max_epochs': 40, 'lr': 0.01, 'batch_size': 256}

"""

"""
   Create the Data Generator for Main Training Process by Collecting Training Data of each Device:
"""
x = ''
batch_size = 256
def train_data_loaders(batch_size = batch_size):
    data_1 = torch.utils.data.DataLoader(benign_train_1, batch_size = batch_size, shuffle=True)
    data_2 = torch.utils.data.DataLoader(benign_train_2, batch_size = batch_size, shuffle=True)
    data_3 = torch.utils.data.DataLoader(benign_train_3, batch_size = batch_size, shuffle=True)
    data_4 = torch.utils.data.DataLoader(benign_train_4, batch_size = batch_size, shuffle=True)
    data_5 = torch.utils.data.DataLoader(benign_train_5, batch_size = batch_size, shuffle=True)
    data_6 = torch.utils.data.DataLoader(benign_train_6, batch_size = batch_size, shuffle=True)
    data_7 = torch.utils.data.DataLoader(benign_train_7, batch_size = batch_size, shuffle=True)
    data_8 = torch.utils.data.DataLoader(benign_train_8, batch_size = batch_size, shuffle=True)
    data_9 = torch.utils.data.DataLoader(benign_train_9, batch_size = batch_size, shuffle=True)

    client_loaders = {'D1': data_1,
                      'D2': data_2,
                      'D3': data_3,
                      'D4': data_4,
                      'D5': data_5,
                      'D6': data_6,
                      'D7': data_7,
                      'D8': data_8,
                      'D9': data_9}

    return client_loaders

"""
   Create the Data Generator for Retraining Process by Picking Baseline Data for each Device:
"""
baseline_num = 1000  # choose some data from the train set to retrain the data from trained model
def baseline_data(size = baseline_num, batch_size = batch_size):
    rng = np.random.default_rng()
    x_1 = rng.choice(benign_train_1, size=size, replace = True, shuffle = True, axis=0) # [[[[[[randomly select]]]]]]
    x_2 = rng.choice(benign_train_2, size=size, replace = True, shuffle = True, axis=0)
    x_3 = rng.choice(benign_train_3, size=size, replace = True, shuffle = True, axis=0)
    x_4 = rng.choice(benign_train_4, size=size, replace = True, shuffle = True, axis=0)
    x_5 = rng.choice(benign_train_5, size=size, replace = True, shuffle = True, axis=0)
    x_6 = rng.choice(benign_train_6, size=size, replace = True, shuffle = True, axis=0)
    x_7 = rng.choice(benign_train_7, size=size, replace = True, shuffle = True, axis=0)
    x_8 = rng.choice(benign_train_8, size=size, replace = True, shuffle = True, axis=0)
    x_9 = rng.choice(benign_train_9, size=size, replace = True, shuffle = True, axis=0)


    loader_1 = torch.utils.data.DataLoader(x_1, batch_size=batch_size)
    loader_2 = torch.utils.data.DataLoader(x_2, batch_size=batch_size)
    loader_3 = torch.utils.data.DataLoader(x_3, batch_size=batch_size)
    loader_4 = torch.utils.data.DataLoader(x_4, batch_size=batch_size)
    loader_5 = torch.utils.data.DataLoader(x_5, batch_size=batch_size)
    loader_6 = torch.utils.data.DataLoader(x_6, batch_size=batch_size)
    loader_7 = torch.utils.data.DataLoader(x_7, batch_size=batch_size)
    loader_8 = torch.utils.data.DataLoader(x_8, batch_size=batch_size)
    loader_9 = torch.utils.data.DataLoader(x_9, batch_size=batch_size)

    loaders = {'D1': loader_1,
               'D2': loader_2,
               'D3': loader_3,
               'D4': loader_4,
               'D5': loader_5,
               'D6': loader_6,
               'D7': loader_7,
               'D8': loader_8,
               'D9': loader_9}

    return loaders

"""
    Define Other Training Parameters, Initialize the Model, and Data:
"""
num_clients = 9      # Number of clients
num_selected = 9     # Typically, num_selected is around 30–40% of the num_clients.
num_rounds = 1       # Total number of communication rounds for the global model to train.
epochs = 40           # for train client model
retrain_epochs = 5   # Total number of retraining rounds on the global server after receiving the model weights
                     # from all the clients that participated in the communication round.

global_model = AEModel().to(device) # for example
client_models = [AEModel().to(device) for _ in range(num_selected)]
print(client_models)

for model in client_models:
    model.load_state_dict(global_model.state_dict())

train_loader = train_data_loaders()
print(train_loader)

"""
   Initialize the Optimizer, Data, Devices:
"""
opt = [torch.optim.SGD(model.parameters(), lr = 0.01, weight_decay=1e-05, momentum=0.9) for model in client_models]
baseline_data = baseline_data()
train_loader = train_data_loaders()
devices = ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9'] # All devices

"""
   Train The Federated Model:
"""
# Train Client Model and Global Model
train_loss_client = []
train_loss_global = []

start_time = time.time()

for r in range(num_rounds):

    print('\nround: ', r + 1)
    client_idx = np.random.permutation(num_clients)[:num_selected] # random pick some clients from all clients for train
    print('client_idx: ', client_idx)
    client_lens = [len(train_loader[devices[idx]]) for idx in client_idx ]
    print(client_lens)

    # Update All Clients:
    loss = 0
    for i in tqdm(range(num_selected)):
        client_syn(client_models[i], global_model)
        loss += client_update(client_models[i], opt[i], train_loader[devices[client_idx[i]]], epochs)
    train_loss_client.append(loss)

    # Retrain the Model on the Global Server:
    loss_retrain = 0
    for i in tqdm(range(num_selected)):
        loss_retrain += client_update(client_models[i], opt[i], baseline_data[devices[client_idx[i]]], retrain_epochs)
    train_loss_global.append(loss_retrain/num_selected)

    # Aggregate all Client Models:
    server_aggregate_M(global_model, client_models, client_lens)

    print("\nclient_loss: ", loss)
    print('global_loss: ', loss_retrain/num_selected)

# save model
torch.save(global_model, 'global_AE.pt')

time_required = time.time() - start_time
print('\nTIME: {}mins'.format(time_required/60))

"""
Load the Trained Global Model - AutoEncoder Model
"""
global_model = torch.load('global_AE.pt')

"""
    Test on Client Devices - First Calculate the Threshold, and then Test on the devices:
"""
performance = {}

# Device 1:
thr_1 = get_tr(global_model, benign_tr_1)
print("D1: ")
performance['D1'] = get_mix_result(global_model, thr_1, mix_data_1, mix_label_1)

print(x)

# Device 2:
thr_2 = get_tr(global_model, benign_tr_2)
print("D2: ")
performance['D2'] = get_mix_result(global_model, thr_2, mix_data_2, mix_label_2)

print(x)

# Device 3:
thr_3 = get_tr(global_model, benign_tr_3)
print("D3: ")
performance['D3'] = get_mix_result(global_model, thr_3, mix_data_3, mix_label_3)

print(x)

# Device 4:
thr_4 = get_tr(global_model, benign_tr_4)
print("D4: ")
performance['D4'] = get_mix_result(global_model, thr_4, mix_data_4, mix_label_4)

print(x)

# Device 5:
thr_5 = get_tr(global_model, benign_tr_5)
print("D5: ")
performance['D5'] = get_mix_result(global_model, thr_5, mix_data_5, mix_label_5)

print(x)

# Device 6:
thr_6 = get_tr(global_model, benign_tr_6)
print("D6: ")
performance['D6'] = get_mix_result(global_model, thr_6, mix_data_6, mix_label_6)

print(x)

# Device 7:
thr_7 = get_tr(global_model, benign_tr_7)
print("D7: ")
performance['D7'] = get_mix_result(global_model, thr_7, mix_data_7, mix_label_7)

print(x)

# Device 8:
thr_8 = get_tr(global_model, benign_tr_8)
print("D8: ")
performance['D8'] = get_mix_result(global_model, thr_8, mix_data_8, mix_label_8)

print(x)

# Device 9:
thr_9 = get_tr(global_model, benign_tr_9)
print("D9: ")
performance['D9'] = get_mix_result(global_model, thr_9, mix_data_9, mix_label_9)

"""
    Create the Data Generator for Main Training Process by Collecting Training Data of each Device,
    Define Other Training Parameters, Initialize the Model, and Data, Initialize the Optimizer, Data, Devices,
    Train the Non-Federated Model, Load Trained Model, and Test on Client's Devices:

    Using Device 1 Optimized Parameters for the AE Model -  Non- Federated Learning:
    {'optimizer': <class 'torch.optim.sgd.SGD'>, 'max_epochs': 40, 'lr': 0.01, 'batch_size': 256}

"""

"""
   Create the Data Generator for Main Training Process by Collecting Training Data of each Device:
"""
batch_size = 256
def train_data_loaders(batch_size = batch_size):
    data_1 = torch.utils.data.DataLoader(benign_train_1, batch_size = batch_size, shuffle=True)
    return data_1


"""
    Define Other Training Parameters, Initialize the Model, and Data:
"""
epochs = 40           # for train client model

# Model Initialization
model = AEModel().to(device) # for example

train_loader = train_data_loaders()
print(train_loader)

"""
   Initialize the Optimizer, Data, Devices:
"""
opt = torch.optim.SGD(model.parameters(), lr = 0.01, weight_decay=1e-05, momentum=0.9)
train_loader = train_data_loaders()

"""
   Train The Non- Federated Model:
"""

# Train Non-FL Model
train_loss_client = []

start_time = time.time()

model.train()
for e in range(epochs):
    running_loss = 0.0
    for bx, (data) in enumerate(train_loader):
        output = model(data.float())
        opt.zero_grad()
        loss = nn.MSELoss(reduction='mean')(data.float().to(device), output)
        loss.backward()
        opt.step()
        # print(loss.item())
        running_loss += loss.item()
    # print(running_loss)
    epoch_loss = running_loss/len(train_loader)

# save model
torch.save(model, 'Non_FL_AE.pt')

time_required = time.time() - start_time
print('\nTIME: {}mins'.format(time_required/60))

"""
Load the Trained Non-FL Model - AutoEncoder Model
"""
model = torch.load('Non_FL_AE.pt')

"""
    Test on Client Devices - First Calculate the Threshold, and then Test on the devices:
"""
performance = {}

# Device 1:
thr_1 = get_tr(model, benign_tr_1)
print("D1: ")
performance['D1'] = get_mix_result(model, thr_1, mix_data_1, mix_label_1)

print(x)

# Device 2:
thr_2 = get_tr(model, benign_tr_2)
print("D2: ")
performance['D2'] = get_mix_result(model, thr_2, mix_data_2, mix_label_2)

print(x)

# Device 3:
thr_3 = get_tr(model, benign_tr_3)
print("D3: ")
performance['D3'] = get_mix_result(model, thr_3, mix_data_3, mix_label_3)

print(x)

# Device 4:
thr_4 = get_tr(model, benign_tr_4)
print("D4: ")
performance['D4'] = get_mix_result(model, thr_4, mix_data_4, mix_label_4)

print(x)

# Device 5:
thr_5 = get_tr(model, benign_tr_5)
print("D5: ")
performance['D5'] = get_mix_result(model, thr_5, mix_data_5, mix_label_5)

print(x)

# Device 6:
thr_6 = get_tr(model, benign_tr_6)
print("D6: ")
performance['D6'] = get_mix_result(model, thr_6, mix_data_6, mix_label_6)

print(x)

# Device 7:
thr_7 = get_tr(model, benign_tr_7)
print("D7: ")
performance['D7'] = get_mix_result(model, thr_7, mix_data_7, mix_label_7)

print(x)

# Device 8:
thr_8 = get_tr(model, benign_tr_8)
print("D8: ")
performance['D8'] = get_mix_result(model, thr_8, mix_data_8, mix_label_8)

print(x)

# Device 9:
thr_9 = get_tr(model, benign_tr_9)
print("D9: ")
performance['D9'] = get_mix_result(model, thr_9, mix_data_9, mix_label_9)

"""
    Create the Data Generator for Retraining Process by Picking Baseline Data for each Device,
    Create the Data Generator for Retraining Process by Picking Baseline Data for each Device,
    Define Other Training Parameters, Initialize the Model, and Data, Initialize the Optimizer, Data, Devices,
    Train the Federated Model, Load Trained Model, and Test on Client's Devices:

    Using Device 2 Optimized Parameters for the AE Model:
    {'optimizer': <class 'torch.optim.adam.Adam'>, 'max_epochs': 20, 'lr': 0.01, 'batch_size': 64}

"""

"""
   Create the Data Generator for Main Training Process by Collecting Training Data of each Device:
"""
batch_size = 64
def train_data_loaders(batch_size = batch_size):
    data_1 = torch.utils.data.DataLoader(benign_train_1, batch_size = batch_size, shuffle=True)
    data_2 = torch.utils.data.DataLoader(benign_train_2, batch_size = batch_size, shuffle=True)
    data_3 = torch.utils.data.DataLoader(benign_train_3, batch_size = batch_size, shuffle=True)
    data_4 = torch.utils.data.DataLoader(benign_train_4, batch_size = batch_size, shuffle=True)
    data_5 = torch.utils.data.DataLoader(benign_train_5, batch_size = batch_size, shuffle=True)
    data_6 = torch.utils.data.DataLoader(benign_train_6, batch_size = batch_size, shuffle=True)
    data_7 = torch.utils.data.DataLoader(benign_train_7, batch_size = batch_size, shuffle=True)
    data_8 = torch.utils.data.DataLoader(benign_train_8, batch_size = batch_size, shuffle=True)
    data_9 = torch.utils.data.DataLoader(benign_train_9, batch_size = batch_size, shuffle=True)

    client_loaders = {'D1': data_1,
                      'D2': data_2,
                      'D3': data_3,
                      'D4': data_4,
                      'D5': data_5,
                      'D6': data_6,
                      'D7': data_7,
                      'D8': data_8,
                      'D9': data_9}

    return client_loaders

"""
   Create the Data Generator for Retraining Process by Picking Baseline Data for each Device:
"""
baseline_num = 1000  # choose some data from the train set to retrain the data from trained model
def baseline_data(size = baseline_num, batch_size = batch_size):
    rng = np.random.default_rng()
    x_1 = rng.choice(benign_train_1, size=size, replace = True, shuffle = True, axis=0) # [[[[[[randomly select]]]]]]
    x_2 = rng.choice(benign_train_2, size=size, replace = True, shuffle = True, axis=0)
    x_3 = rng.choice(benign_train_3, size=size, replace = True, shuffle = True, axis=0)
    x_4 = rng.choice(benign_train_4, size=size, replace = True, shuffle = True, axis=0)
    x_5 = rng.choice(benign_train_5, size=size, replace = True, shuffle = True, axis=0)
    x_6 = rng.choice(benign_train_6, size=size, replace = True, shuffle = True, axis=0)
    x_7 = rng.choice(benign_train_7, size=size, replace = True, shuffle = True, axis=0)
    x_8 = rng.choice(benign_train_8, size=size, replace = True, shuffle = True, axis=0)
    x_9 = rng.choice(benign_train_9, size=size, replace = True, shuffle = True, axis=0)


    loader_1 = torch.utils.data.DataLoader(x_1, batch_size=batch_size)
    loader_2 = torch.utils.data.DataLoader(x_2, batch_size=batch_size)
    loader_3 = torch.utils.data.DataLoader(x_3, batch_size=batch_size)
    loader_4 = torch.utils.data.DataLoader(x_4, batch_size=batch_size)
    loader_5 = torch.utils.data.DataLoader(x_5, batch_size=batch_size)
    loader_6 = torch.utils.data.DataLoader(x_6, batch_size=batch_size)
    loader_7 = torch.utils.data.DataLoader(x_7, batch_size=batch_size)
    loader_8 = torch.utils.data.DataLoader(x_8, batch_size=batch_size)
    loader_9 = torch.utils.data.DataLoader(x_9, batch_size=batch_size)

    loaders = {'D1': loader_1,
               'D2': loader_2,
               'D3': loader_3,
               'D4': loader_4,
               'D5': loader_5,
               'D6': loader_6,
               'D7': loader_7,
               'D8': loader_8,
               'D9': loader_9}

    return loaders

"""
    Define Other Training Parameters, Initialize the Model, and Data:
"""
num_clients = 9      # Number of clients
num_selected = 9     # Typically, num_selected is around 30–40% of the num_clients.
num_rounds = 10       # Total number of communication rounds for the global model to train.
epochs = 20           # for train client model
retrain_epochs = 5   # Total number of retraining rounds on the global server after receiving the model weights
                     # from all the clients that participated in the communication round.

global_model = AEModel().to(device) # for example
client_models = [AEModel().to(device) for _ in range(num_selected)]
print(client_models)

for model in client_models:
    model.load_state_dict(global_model.state_dict())

train_loader = train_data_loaders()
print(train_loader)

"""
   Initialize the Optimizer, Data, Devices:
"""
opt = [torch.optim.Adam(model.parameters(), lr = 0.01, weight_decay=1e-05) for model in client_models]
baseline_data = baseline_data()
train_loader = train_data_loaders()
devices = ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9'] # All devices

"""
   Train The Federated Model:
"""
# Train Client Model and Global Model
train_loss_client = []
train_loss_global = []

start_time = time.time()

for r in range(num_rounds):

    print('\nround: ', r + 1)
    client_idx = np.random.permutation(num_clients)[:num_selected] # random pick some clients from all clients for train
    print('client_idx: ', client_idx)
    client_lens = [len(train_loader[devices[idx]]) for idx in client_idx ]
    print(client_lens)

    # Update All Clients:
    loss = 0
    for i in tqdm(range(num_selected)):
        client_syn(client_models[i], global_model)
        loss += client_update(client_models[i], opt[i], train_loader[devices[client_idx[i]]], epochs)
    train_loss_client.append(loss)

    # Retrain the Model on the Global Server:
    loss_retrain = 0
    for i in tqdm(range(num_selected)):
        loss_retrain += client_update(client_models[i], opt[i], baseline_data[devices[client_idx[i]]], retrain_epochs)
    train_loss_global.append(loss_retrain/num_selected)

    # Aggregate all Client Models:
    server_aggregate_M(global_model, client_models, client_lens)

    print("\nclient_loss: ", loss)
    print('global_loss: ', loss_retrain/num_selected)

# save model
torch.save(global_model, 'global_AE.pt')

time_required = time.time() - start_time
print('\nTIME: {}mins'.format(time_required/60))

"""
Load the Trained Global Model - AutoEncoder Model
"""
global_model = torch.load('global_AE.pt')

"""
    Test on Client Devices - First Calculate the Threshold, and then Test on the devices:
"""
performance = {}

# Device 1:
thr_1 = get_tr(global_model, benign_tr_1)
print("D1: ")
performance['D1'] = get_mix_result(global_model, thr_1, mix_data_1, mix_label_1)

print(x)

# Device 2:
thr_2 = get_tr(global_model, benign_tr_2)
print("D2: ")
performance['D2'] = get_mix_result(global_model, thr_2, mix_data_2, mix_label_2)

print(x)

# Device 3:
thr_3 = get_tr(global_model, benign_tr_3)
print("D3: ")
performance['D3'] = get_mix_result(global_model, thr_3, mix_data_3, mix_label_3)

print(x)

# Device 4:
thr_4 = get_tr(global_model, benign_tr_4)
print("D4: ")
performance['D4'] = get_mix_result(global_model, thr_4, mix_data_4, mix_label_4)

print(x)

# Device 5:
thr_5 = get_tr(global_model, benign_tr_5)
print("D5: ")
performance['D5'] = get_mix_result(global_model, thr_5, mix_data_5, mix_label_5)

print(x)

# Device 6:
thr_6 = get_tr(global_model, benign_tr_6)
print("D6: ")
performance['D6'] = get_mix_result(global_model, thr_6, mix_data_6, mix_label_6)

print(x)

# Device 7:
thr_7 = get_tr(global_model, benign_tr_7)
print("D7: ")
performance['D7'] = get_mix_result(global_model, thr_7, mix_data_7, mix_label_7)

print(x)

# Device 8:
thr_8 = get_tr(global_model, benign_tr_8)
print("D8: ")
performance['D8'] = get_mix_result(global_model, thr_8, mix_data_8, mix_label_8)

print(x)

# Device 9:
thr_9 = get_tr(global_model, benign_tr_9)
print("D9: ")
performance['D9'] = get_mix_result(global_model, thr_9, mix_data_9, mix_label_9)

"""
    Create the Data Generator for Main Training Process by Collecting Training Data of each Device,
    Define Other Training Parameters, Initialize the Model, and Data, Initialize the Optimizer, Data, Devices,
    Train the Non-Federated Model, Load Trained Model, and Test on Client's Devices:

    Using Device 2 Optimized Parameters for the AE Model -  Non- Federated Learning:
    {'optimizer': <class 'torch.optim.adam.Adam'>, 'max_epochs': 20, 'lr': 0.01, 'batch_size': 64}

"""

"""
   Create the Data Generator for Main Training Process by Collecting Training Data of each Device:
"""
batch_size = 64
def train_data_loaders(batch_size = batch_size):
    data_2 = torch.utils.data.DataLoader(benign_train_2, batch_size = batch_size, shuffle=True)
    return data_2


"""
    Define Other Training Parameters, Initialize the Model, and Data:
"""
epochs = 20           # for train client model

# Model Initialization
model = AEModel().to(device) # for example

train_loader = train_data_loaders()
print(train_loader)

"""
   Initialize the Optimizer, Data, Devices:
"""
opt = torch.optim.Adam(model.parameters(), lr = 0.01, weight_decay=1e-05)
train_loader = train_data_loaders()

"""
   Train The Non- Federated Model:
"""

# Train Non-FL Model
train_loss_client = []

start_time = time.time()

model.train()
for e in range(epochs):
    running_loss = 0.0
    for bx, (data) in enumerate(train_loader):
        output = model(data.float())
        opt.zero_grad()
        loss = nn.MSELoss(reduction='mean')(data.float().to(device), output)
        loss.backward()
        opt.step()
        # print(loss.item())
        running_loss += loss.item()
    # print(running_loss)
    epoch_loss = running_loss/len(train_loader)

# save model
torch.save(model, 'Non_FL_AE.pt')

time_required = time.time() - start_time
print('\nTIME: {}mins'.format(time_required/60))

"""
Load the Trained Non-FL Model - AutoEncoder Model
"""
model = torch.load('Non_FL_AE.pt')

"""
    Test on Client Devices - First Calculate the Threshold, and then Test on the devices:
"""
performance = {}

# Device 1:
thr_1 = get_tr(model, benign_tr_1)
print("D1: ")
performance['D1'] = get_mix_result(model, thr_1, mix_data_1, mix_label_1)

print(x)

# Device 2:
thr_2 = get_tr(model, benign_tr_2)
print("D2: ")
performance['D2'] = get_mix_result(model, thr_2, mix_data_2, mix_label_2)

print(x)

# Device 3:
thr_3 = get_tr(model, benign_tr_3)
print("D3: ")
performance['D3'] = get_mix_result(model, thr_3, mix_data_3, mix_label_3)

print(x)

# Device 4:
thr_4 = get_tr(model, benign_tr_4)
print("D4: ")
performance['D4'] = get_mix_result(model, thr_4, mix_data_4, mix_label_4)

print(x)

# Device 5:
thr_5 = get_tr(model, benign_tr_5)
print("D5: ")
performance['D5'] = get_mix_result(model, thr_5, mix_data_5, mix_label_5)

print(x)

# Device 6:
thr_6 = get_tr(model, benign_tr_6)
print("D6: ")
performance['D6'] = get_mix_result(model, thr_6, mix_data_6, mix_label_6)

print(x)

# Device 7:
thr_7 = get_tr(model, benign_tr_7)
print("D7: ")
performance['D7'] = get_mix_result(model, thr_7, mix_data_7, mix_label_7)

print(x)

# Device 8:
thr_8 = get_tr(model, benign_tr_8)
print("D8: ")
performance['D8'] = get_mix_result(model, thr_8, mix_data_8, mix_label_8)

print(x)

# Device 9:
thr_9 = get_tr(model, benign_tr_9)
print("D9: ")
performance['D9'] = get_mix_result(model, thr_9, mix_data_9, mix_label_9)

"""
    Create the Data Generator for Retraining Process by Picking Baseline Data for each Device,
    Create the Data Generator for Retraining Process by Picking Baseline Data for each Device,
    Define Other Training Parameters, Initialize the Model, and Data, Initialize the Optimizer, Data, Devices,
    Train the Federated Model, Load Trained Model, and Test on Client's Devices:

    Using Device 3 Optimized Parameters for the AE Model:
    {'optimizer': <class 'torch.optim.sgd.SGD'>, 'max_epochs': 40, 'lr': 0.001, 'batch_size': 64}
"""

"""
   Create the Data Generator for Main Training Process by Collecting Training Data of each Device:
"""
batch_size = 64
def train_data_loaders(batch_size = batch_size):
    data_1 = torch.utils.data.DataLoader(benign_train_1, batch_size = batch_size, shuffle=True)
    data_2 = torch.utils.data.DataLoader(benign_train_2, batch_size = batch_size, shuffle=True)
    data_3 = torch.utils.data.DataLoader(benign_train_3, batch_size = batch_size, shuffle=True)
    data_4 = torch.utils.data.DataLoader(benign_train_4, batch_size = batch_size, shuffle=True)
    data_5 = torch.utils.data.DataLoader(benign_train_5, batch_size = batch_size, shuffle=True)
    data_6 = torch.utils.data.DataLoader(benign_train_6, batch_size = batch_size, shuffle=True)
    data_7 = torch.utils.data.DataLoader(benign_train_7, batch_size = batch_size, shuffle=True)
    data_8 = torch.utils.data.DataLoader(benign_train_8, batch_size = batch_size, shuffle=True)
    data_9 = torch.utils.data.DataLoader(benign_train_9, batch_size = batch_size, shuffle=True)

    client_loaders = {'D1': data_1,
                      'D2': data_2,
                      'D3': data_3,
                      'D4': data_4,
                      'D5': data_5,
                      'D6': data_6,
                      'D7': data_7,
                      'D8': data_8,
                      'D9': data_9}

    return client_loaders

"""
   Create the Data Generator for Retraining Process by Picking Baseline Data for each Device:
"""
baseline_num = 1000  # choose some data from the train set to retrain the data from trained model
def baseline_data(size = baseline_num, batch_size = batch_size):
    rng = np.random.default_rng()
    x_1 = rng.choice(benign_train_1, size=size, replace = True, shuffle = True, axis=0) # [[[[[[randomly select]]]]]]
    x_2 = rng.choice(benign_train_2, size=size, replace = True, shuffle = True, axis=0)
    x_3 = rng.choice(benign_train_3, size=size, replace = True, shuffle = True, axis=0)
    x_4 = rng.choice(benign_train_4, size=size, replace = True, shuffle = True, axis=0)
    x_5 = rng.choice(benign_train_5, size=size, replace = True, shuffle = True, axis=0)
    x_6 = rng.choice(benign_train_6, size=size, replace = True, shuffle = True, axis=0)
    x_7 = rng.choice(benign_train_7, size=size, replace = True, shuffle = True, axis=0)
    x_8 = rng.choice(benign_train_8, size=size, replace = True, shuffle = True, axis=0)
    x_9 = rng.choice(benign_train_9, size=size, replace = True, shuffle = True, axis=0)


    loader_1 = torch.utils.data.DataLoader(x_1, batch_size=batch_size)
    loader_2 = torch.utils.data.DataLoader(x_2, batch_size=batch_size)
    loader_3 = torch.utils.data.DataLoader(x_3, batch_size=batch_size)
    loader_4 = torch.utils.data.DataLoader(x_4, batch_size=batch_size)
    loader_5 = torch.utils.data.DataLoader(x_5, batch_size=batch_size)
    loader_6 = torch.utils.data.DataLoader(x_6, batch_size=batch_size)
    loader_7 = torch.utils.data.DataLoader(x_7, batch_size=batch_size)
    loader_8 = torch.utils.data.DataLoader(x_8, batch_size=batch_size)
    loader_9 = torch.utils.data.DataLoader(x_9, batch_size=batch_size)

    loaders = {'D1': loader_1,
               'D2': loader_2,
               'D3': loader_3,
               'D4': loader_4,
               'D5': loader_5,
               'D6': loader_6,
               'D7': loader_7,
               'D8': loader_8,
               'D9': loader_9}

    return loaders

"""
    Define Other Training Parameters, Initialize the Model, and Data:
"""
num_clients = 9      # Number of clients
num_selected = 9     # Typically, num_selected is around 30–40% of the num_clients.
num_rounds = 10       # Total number of communication rounds for the global model to train.
epochs = 40           # for train client model
retrain_epochs = 5   # Total number of retraining rounds on the global server after receiving the model weights
                     # from all the clients that participated in the communication round.

global_model = AEModel().to(device) # for example
client_models = [AEModel().to(device) for _ in range(num_selected)]
print(client_models)

for model in client_models:
    model.load_state_dict(global_model.state_dict())

train_loader = train_data_loaders()
print(train_loader)

"""
   Initialize the Optimizer, Data, Devices:
"""
opt = [torch.optim.SGD(model.parameters(), lr = 0.001, weight_decay=1e-05, momentum=0.9) for model in client_models]
baseline_data = baseline_data()
train_loader = train_data_loaders()
devices = ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9'] # All devices

"""
   Train The Federated Model:
"""
# Train Client Model and Global Model
train_loss_client = []
train_loss_global = []

start_time = time.time()

for r in range(num_rounds):

    print('\nround: ', r + 1)
    client_idx = np.random.permutation(num_clients)[:num_selected] # random pick some clients from all clients for train
    print('client_idx: ', client_idx)
    client_lens = [len(train_loader[devices[idx]]) for idx in client_idx ]
    print(client_lens)

    # Update All Clients:
    loss = 0
    for i in tqdm(range(num_selected)):
        client_syn(client_models[i], global_model)
        loss += client_update(client_models[i], opt[i], train_loader[devices[client_idx[i]]], epochs)
    train_loss_client.append(loss)

    # Retrain the Model on the Global Server:
    loss_retrain = 0
    for i in tqdm(range(num_selected)):
        loss_retrain += client_update(client_models[i], opt[i], baseline_data[devices[client_idx[i]]], retrain_epochs)
    train_loss_global.append(loss_retrain/num_selected)

    # Aggregate all Client Models:
    server_aggregate_M(global_model, client_models, client_lens)

    print("\nclient_loss: ", loss)
    print('global_loss: ', loss_retrain/num_selected)

# save model
torch.save(global_model, 'global_AE.pt')

time_required = time.time() - start_time
print('\nTIME: {}mins'.format(time_required/60))

"""
Load the Trained Global Model - AutoEncoder Model
"""
global_model = torch.load('global_AE.pt')

"""
    Test on Client Devices - First Calculate the Threshold, and then Test on the devices:
"""
performance = {}

# Device 1:
thr_1 = get_tr(global_model, benign_tr_1)
print("D1: ")
performance['D1'] = get_mix_result(global_model, thr_1, mix_data_1, mix_label_1)

print(x)

# Device 2:
thr_2 = get_tr(global_model, benign_tr_2)
print("D2: ")
performance['D2'] = get_mix_result(global_model, thr_2, mix_data_2, mix_label_2)

print(x)

# Device 3:
thr_3 = get_tr(global_model, benign_tr_3)
print("D3: ")
performance['D3'] = get_mix_result(global_model, thr_3, mix_data_3, mix_label_3)

print(x)

# Device 4:
thr_4 = get_tr(global_model, benign_tr_4)
print("D4: ")
performance['D4'] = get_mix_result(global_model, thr_4, mix_data_4, mix_label_4)

print(x)

# Device 5:
thr_5 = get_tr(global_model, benign_tr_5)
print("D5: ")
performance['D5'] = get_mix_result(global_model, thr_5, mix_data_5, mix_label_5)

print(x)

# Device 6:
thr_6 = get_tr(global_model, benign_tr_6)
print("D6: ")
performance['D6'] = get_mix_result(global_model, thr_6, mix_data_6, mix_label_6)

print(x)

# Device 7:
thr_7 = get_tr(global_model, benign_tr_7)
print("D7: ")
performance['D7'] = get_mix_result(global_model, thr_7, mix_data_7, mix_label_7)

print(x)

# Device 8:
thr_8 = get_tr(global_model, benign_tr_8)
print("D8: ")
performance['D8'] = get_mix_result(global_model, thr_8, mix_data_8, mix_label_8)

print(x)

# Device 9:
thr_9 = get_tr(global_model, benign_tr_9)
print("D9: ")
performance['D9'] = get_mix_result(global_model, thr_9, mix_data_9, mix_label_9)

"""
    Create the Data Generator for Main Training Process by Collecting Training Data of each Device,
    Define Other Training Parameters, Initialize the Model, and Data, Initialize the Optimizer, Data, Devices,
    Train the Non-Federated Model, Load Trained Model, and Test on Client's Devices:

    Using Device 3 Optimized Parameters for the AE Model -  Non- Federated Learning:
    {'optimizer': <class 'torch.optim.sgd.SGD'>, 'max_epochs': 40, 'lr': 0.001, 'batch_size': 64}

"""

"""
   Create the Data Generator for Main Training Process by Collecting Training Data of each Device:
"""
batch_size = 64
def train_data_loaders(batch_size = batch_size):
    data_3 = torch.utils.data.DataLoader(benign_train_3, batch_size = batch_size, shuffle=True)
    return data_3


"""
    Define Other Training Parameters, Initialize the Model, and Data:
"""
epochs = 40           # for train client model

# Model Initialization
model = AEModel().to(device) # for example

train_loader = train_data_loaders()
print(train_loader)

"""
   Initialize the Optimizer, Data, Devices:
"""
opt = torch.optim.SGD(model.parameters(), lr = 0.001, weight_decay=1e-05, momentum=0.9)
train_loader = train_data_loaders()

"""
   Train The Non- Federated Model:
"""

# Train Non-FL Model
train_loss_client = []

start_time = time.time()

model.train()
for e in range(epochs):
    running_loss = 0.0
    for bx, (data) in enumerate(train_loader):
        output = model(data.float())
        opt.zero_grad()
        loss = nn.MSELoss(reduction='mean')(data.float().to(device), output)
        loss.backward()
        opt.step()
        # print(loss.item())
        running_loss += loss.item()
    # print(running_loss)
    epoch_loss = running_loss/len(train_loader)

# save model
torch.save(model, 'Non_FL_AE.pt')

time_required = time.time() - start_time
print('\nTIME: {}mins'.format(time_required/60))

"""
Load the Trained Non-FL Model - AutoEncoder Model
"""
model = torch.load('Non_FL_AE.pt')

"""
    Test on Client Devices - First Calculate the Threshold, and then Test on the devices:
"""
performance = {}

# Device 1:
thr_1 = get_tr(model, benign_tr_1)
print("D1: ")
performance['D1'] = get_mix_result(model, thr_1, mix_data_1, mix_label_1)

print(x)

# Device 2:
thr_2 = get_tr(model, benign_tr_2)
print("D2: ")
performance['D2'] = get_mix_result(model, thr_2, mix_data_2, mix_label_2)

print(x)

# Device 3:
thr_3 = get_tr(model, benign_tr_3)
print("D3: ")
performance['D3'] = get_mix_result(model, thr_3, mix_data_3, mix_label_3)

print(x)

# Device 4:
thr_4 = get_tr(model, benign_tr_4)
print("D4: ")
performance['D4'] = get_mix_result(model, thr_4, mix_data_4, mix_label_4)

print(x)

# Device 5:
thr_5 = get_tr(model, benign_tr_5)
print("D5: ")
performance['D5'] = get_mix_result(model, thr_5, mix_data_5, mix_label_5)

print(x)

# Device 6:
thr_6 = get_tr(model, benign_tr_6)
print("D6: ")
performance['D6'] = get_mix_result(model, thr_6, mix_data_6, mix_label_6)

print(x)

# Device 7:
thr_7 = get_tr(model, benign_tr_7)
print("D7: ")
performance['D7'] = get_mix_result(model, thr_7, mix_data_7, mix_label_7)

print(x)

# Device 8:
thr_8 = get_tr(model, benign_tr_8)
print("D8: ")
performance['D8'] = get_mix_result(model, thr_8, mix_data_8, mix_label_8)

print(x)

# Device 9:
thr_9 = get_tr(model, benign_tr_9)
print("D9: ")
performance['D9'] = get_mix_result(model, thr_9, mix_data_9, mix_label_9)

"""
    Create the Data Generator for Retraining Process by Picking Baseline Data for each Device,
    Create the Data Generator for Retraining Process by Picking Baseline Data for each Device,
    Define Other Training Parameters, Initialize the Model, and Data, Initialize the Optimizer, Data, Devices,
    Train the Federated Model, Load Trained Model, and Test on Client's Devices:

    Using Device 4 Optimized Parameters for the AE Model:
    {'optimizer': <class 'torch.optim.adam.Adam'>, 'max_epochs': 40, 'lr': 0.0001, 'batch_size': 256}
"""

"""
   Create the Data Generator for Main Training Process by Collecting Training Data of each Device:
"""
batch_size = 256
def train_data_loaders(batch_size = batch_size):
    data_1 = torch.utils.data.DataLoader(benign_train_1, batch_size = batch_size, shuffle=True)
    data_2 = torch.utils.data.DataLoader(benign_train_2, batch_size = batch_size, shuffle=True)
    data_3 = torch.utils.data.DataLoader(benign_train_3, batch_size = batch_size, shuffle=True)
    data_4 = torch.utils.data.DataLoader(benign_train_4, batch_size = batch_size, shuffle=True)
    data_5 = torch.utils.data.DataLoader(benign_train_5, batch_size = batch_size, shuffle=True)
    data_6 = torch.utils.data.DataLoader(benign_train_6, batch_size = batch_size, shuffle=True)
    data_7 = torch.utils.data.DataLoader(benign_train_7, batch_size = batch_size, shuffle=True)
    data_8 = torch.utils.data.DataLoader(benign_train_8, batch_size = batch_size, shuffle=True)
    data_9 = torch.utils.data.DataLoader(benign_train_9, batch_size = batch_size, shuffle=True)

    client_loaders = {'D1': data_1,
                      'D2': data_2,
                      'D3': data_3,
                      'D4': data_4,
                      'D5': data_5,
                      'D6': data_6,
                      'D7': data_7,
                      'D8': data_8,
                      'D9': data_9}

    return client_loaders

"""
   Create the Data Generator for Retraining Process by Picking Baseline Data for each Device:
"""
baseline_num = 1000  # choose some data from the train set to retrain the data from trained model
def baseline_data(size = baseline_num, batch_size = batch_size):
    rng = np.random.default_rng()
    x_1 = rng.choice(benign_train_1, size=size, replace = True, shuffle = True, axis=0) # [[[[[[randomly select]]]]]]
    x_2 = rng.choice(benign_train_2, size=size, replace = True, shuffle = True, axis=0)
    x_3 = rng.choice(benign_train_3, size=size, replace = True, shuffle = True, axis=0)
    x_4 = rng.choice(benign_train_4, size=size, replace = True, shuffle = True, axis=0)
    x_5 = rng.choice(benign_train_5, size=size, replace = True, shuffle = True, axis=0)
    x_6 = rng.choice(benign_train_6, size=size, replace = True, shuffle = True, axis=0)
    x_7 = rng.choice(benign_train_7, size=size, replace = True, shuffle = True, axis=0)
    x_8 = rng.choice(benign_train_8, size=size, replace = True, shuffle = True, axis=0)
    x_9 = rng.choice(benign_train_9, size=size, replace = True, shuffle = True, axis=0)


    loader_1 = torch.utils.data.DataLoader(x_1, batch_size=batch_size)
    loader_2 = torch.utils.data.DataLoader(x_2, batch_size=batch_size)
    loader_3 = torch.utils.data.DataLoader(x_3, batch_size=batch_size)
    loader_4 = torch.utils.data.DataLoader(x_4, batch_size=batch_size)
    loader_5 = torch.utils.data.DataLoader(x_5, batch_size=batch_size)
    loader_6 = torch.utils.data.DataLoader(x_6, batch_size=batch_size)
    loader_7 = torch.utils.data.DataLoader(x_7, batch_size=batch_size)
    loader_8 = torch.utils.data.DataLoader(x_8, batch_size=batch_size)
    loader_9 = torch.utils.data.DataLoader(x_9, batch_size=batch_size)

    loaders = {'D1': loader_1,
               'D2': loader_2,
               'D3': loader_3,
               'D4': loader_4,
               'D5': loader_5,
               'D6': loader_6,
               'D7': loader_7,
               'D8': loader_8,
               'D9': loader_9}

    return loaders

"""
    Define Other Training Parameters, Initialize the Model, and Data:
"""
num_clients = 9      # Number of clients
num_selected = 9     # Typically, num_selected is around 30–40% of the num_clients.
num_rounds = 10       # Total number of communication rounds for the global model to train.
epochs = 40           # for train client model
retrain_epochs = 5   # Total number of retraining rounds on the global server after receiving the model weights
                     # from all the clients that participated in the communication round.

global_model = AEModel().to(device) # for example
client_models = [AEModel().to(device) for _ in range(num_selected)]
print(client_models)

for model in client_models:
    model.load_state_dict(global_model.state_dict())

train_loader = train_data_loaders()
print(train_loader)

"""
   Initialize the Optimizer, Data, Devices:
"""
opt = [torch.optim.Adam(model.parameters(), lr = 0.0001, weight_decay=1e-05) for model in client_models]
baseline_data = baseline_data()
train_loader = train_data_loaders()
devices = ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9'] # All devices

"""
   Train The Federated Model:
"""
# Train Client Model and Global Model
train_loss_client = []
train_loss_global = []

start_time = time.time()

for r in range(num_rounds):

    print('\nround: ', r + 1)
    client_idx = np.random.permutation(num_clients)[:num_selected] # random pick some clients from all clients for train
    print('client_idx: ', client_idx)
    client_lens = [len(train_loader[devices[idx]]) for idx in client_idx ]
    print(client_lens)

    # Update All Clients:
    loss = 0
    for i in tqdm(range(num_selected)):
        client_syn(client_models[i], global_model)
        loss += client_update(client_models[i], opt[i], train_loader[devices[client_idx[i]]], epochs)
    train_loss_client.append(loss)

    # Retrain the Model on the Global Server:
    loss_retrain = 0
    for i in tqdm(range(num_selected)):
        loss_retrain += client_update(client_models[i], opt[i], baseline_data[devices[client_idx[i]]], retrain_epochs)
    train_loss_global.append(loss_retrain/num_selected)

    # Aggregate all Client Models:
    server_aggregate_M(global_model, client_models, client_lens)

    print("\nclient_loss: ", loss)
    print('global_loss: ', loss_retrain/num_selected)

# save model
torch.save(global_model, 'global_AE.pt')

time_required = time.time() - start_time
print('\nTIME: {}mins'.format(time_required/60))

"""
Load the Trained Global Model - AutoEncoder Model
"""
global_model = torch.load('global_AE.pt')

"""
    Test on Client Devices - First Calculate the Threshold, and then Test on the devices:
"""
performance = {}

# Device 1:
thr_1 = get_tr(global_model, benign_tr_1)
print("D1: ")
performance['D1'] = get_mix_result(global_model, thr_1, mix_data_1, mix_label_1)

print(x)

# Device 2:
thr_2 = get_tr(global_model, benign_tr_2)
print("D2: ")
performance['D2'] = get_mix_result(global_model, thr_2, mix_data_2, mix_label_2)

print(x)

# Device 3:
thr_3 = get_tr(global_model, benign_tr_3)
print("D3: ")
performance['D3'] = get_mix_result(global_model, thr_3, mix_data_3, mix_label_3)

print(x)

# Device 4:
thr_4 = get_tr(global_model, benign_tr_4)
print("D4: ")
performance['D4'] = get_mix_result(global_model, thr_4, mix_data_4, mix_label_4)

print(x)

# Device 5:
thr_5 = get_tr(global_model, benign_tr_5)
print("D5: ")
performance['D5'] = get_mix_result(global_model, thr_5, mix_data_5, mix_label_5)

print(x)

# Device 6:
thr_6 = get_tr(global_model, benign_tr_6)
print("D6: ")
performance['D6'] = get_mix_result(global_model, thr_6, mix_data_6, mix_label_6)

print(x)

# Device 7:
thr_7 = get_tr(global_model, benign_tr_7)
print("D7: ")
performance['D7'] = get_mix_result(global_model, thr_7, mix_data_7, mix_label_7)

print(x)

# Device 8:
thr_8 = get_tr(global_model, benign_tr_8)
print("D8: ")
performance['D8'] = get_mix_result(global_model, thr_8, mix_data_8, mix_label_8)

print(x)

# Device 9:
thr_9 = get_tr(global_model, benign_tr_9)
print("D9: ")
performance['D9'] = get_mix_result(global_model, thr_9, mix_data_9, mix_label_9)

"""
    Create the Data Generator for Main Training Process by Collecting Training Data of each Device,
    Define Other Training Parameters, Initialize the Model, and Data, Initialize the Optimizer, Data, Devices,
    Train the Non-Federated Model, Load Trained Model, and Test on Client's Devices:

    Using Device 4 Optimized Parameters for the AE Model -  Non- Federated Learning:
    {'optimizer': <class 'torch.optim.adam.Adam'>, 'max_epochs': 40, 'lr': 0.0001, 'batch_size': 256}

"""

"""
   Create the Data Generator for Main Training Process by Collecting Training Data of each Device:
"""
batch_size = 256
def train_data_loaders(batch_size = batch_size):
    data_4 = torch.utils.data.DataLoader(benign_train_4, batch_size = batch_size, shuffle=True)
    return data_4


"""
    Define Other Training Parameters, Initialize the Model, and Data:
"""
epochs = 40           # for train client model

# Model Initialization
model = AEModel().to(device) # for example

train_loader = train_data_loaders()
print(train_loader)

"""
   Initialize the Optimizer, Data, Devices:
"""
opt = torch.optim.Adam(model.parameters(), lr = 0.0001, weight_decay=1e-05)
train_loader = train_data_loaders()

"""
   Train The Non- Federated Model:
"""

# Train Non-FL Model
train_loss_client = []

start_time = time.time()

model.train()
for e in range(epochs):
    running_loss = 0.0
    for bx, (data) in enumerate(train_loader):
        output = model(data.float())
        opt.zero_grad()
        loss = nn.MSELoss(reduction='mean')(data.float().to(device), output)
        loss.backward()
        opt.step()
        # print(loss.item())
        running_loss += loss.item()
    # print(running_loss)
    epoch_loss = running_loss/len(train_loader)

# save model
torch.save(model, 'Non_FL_AE.pt')

time_required = time.time() - start_time
print('\nTIME: {}mins'.format(time_required/60))

"""
Load the Trained Non-FL Model - AutoEncoder Model
"""
model = torch.load('Non_FL_AE.pt')

"""
    Test on Client Devices - First Calculate the Threshold, and then Test on the devices:
"""
performance = {}

# Device 1:
thr_1 = get_tr(model, benign_tr_1)
print("D1: ")
performance['D1'] = get_mix_result(model, thr_1, mix_data_1, mix_label_1)

print(x)

# Device 2:
thr_2 = get_tr(model, benign_tr_2)
print("D2: ")
performance['D2'] = get_mix_result(model, thr_2, mix_data_2, mix_label_2)

print(x)

# Device 3:
thr_3 = get_tr(model, benign_tr_3)
print("D3: ")
performance['D3'] = get_mix_result(model, thr_3, mix_data_3, mix_label_3)

print(x)

# Device 4:
thr_4 = get_tr(model, benign_tr_4)
print("D4: ")
performance['D4'] = get_mix_result(model, thr_4, mix_data_4, mix_label_4)

print(x)

# Device 5:
thr_5 = get_tr(model, benign_tr_5)
print("D5: ")
performance['D5'] = get_mix_result(model, thr_5, mix_data_5, mix_label_5)

print(x)

# Device 6:
thr_6 = get_tr(model, benign_tr_6)
print("D6: ")
performance['D6'] = get_mix_result(model, thr_6, mix_data_6, mix_label_6)

print(x)

# Device 7:
thr_7 = get_tr(model, benign_tr_7)
print("D7: ")
performance['D7'] = get_mix_result(model, thr_7, mix_data_7, mix_label_7)

print(x)

# Device 8:
thr_8 = get_tr(model, benign_tr_8)
print("D8: ")
performance['D8'] = get_mix_result(model, thr_8, mix_data_8, mix_label_8)

print(x)

# Device 9:
thr_9 = get_tr(model, benign_tr_9)
print("D9: ")
performance['D9'] = get_mix_result(model, thr_9, mix_data_9, mix_label_9)

"""
    Create the Data Generator for Retraining Process by Picking Baseline Data for each Device,
    Create the Data Generator for Retraining Process by Picking Baseline Data for each Device,
    Define Other Training Parameters, Initialize the Model, and Data, Initialize the Optimizer, Data, Devices,
    Train the Federated Model, Load Trained Model, and Test on Client's Devices:

    Using Device 5 Optimized Parameters for the AE Model:
    {'optimizer': <class 'torch.optim.sgd.SGD'>, 'max_epochs': 20, 'lr': 0.0001, 'batch_size': 128}
"""

"""
   Create the Data Generator for Main Training Process by Collecting Training Data of each Device:
"""
batch_size = 128
def train_data_loaders(batch_size = batch_size):
    data_1 = torch.utils.data.DataLoader(benign_train_1, batch_size = batch_size, shuffle=True)
    data_2 = torch.utils.data.DataLoader(benign_train_2, batch_size = batch_size, shuffle=True)
    data_3 = torch.utils.data.DataLoader(benign_train_3, batch_size = batch_size, shuffle=True)
    data_4 = torch.utils.data.DataLoader(benign_train_4, batch_size = batch_size, shuffle=True)
    data_5 = torch.utils.data.DataLoader(benign_train_5, batch_size = batch_size, shuffle=True)
    data_6 = torch.utils.data.DataLoader(benign_train_6, batch_size = batch_size, shuffle=True)
    data_7 = torch.utils.data.DataLoader(benign_train_7, batch_size = batch_size, shuffle=True)
    data_8 = torch.utils.data.DataLoader(benign_train_8, batch_size = batch_size, shuffle=True)
    data_9 = torch.utils.data.DataLoader(benign_train_9, batch_size = batch_size, shuffle=True)

    client_loaders = {'D1': data_1,
                      'D2': data_2,
                      'D3': data_3,
                      'D4': data_4,
                      'D5': data_5,
                      'D6': data_6,
                      'D7': data_7,
                      'D8': data_8,
                      'D9': data_9}

    return client_loaders

"""
   Create the Data Generator for Retraining Process by Picking Baseline Data for each Device:
"""
baseline_num = 1000  # choose some data from the train set to retrain the data from trained model
def baseline_data(size = baseline_num, batch_size = batch_size):
    rng = np.random.default_rng()
    x_1 = rng.choice(benign_train_1, size=size, replace = True, shuffle = True, axis=0) # [[[[[[randomly select]]]]]]
    x_2 = rng.choice(benign_train_2, size=size, replace = True, shuffle = True, axis=0)
    x_3 = rng.choice(benign_train_3, size=size, replace = True, shuffle = True, axis=0)
    x_4 = rng.choice(benign_train_4, size=size, replace = True, shuffle = True, axis=0)
    x_5 = rng.choice(benign_train_5, size=size, replace = True, shuffle = True, axis=0)
    x_6 = rng.choice(benign_train_6, size=size, replace = True, shuffle = True, axis=0)
    x_7 = rng.choice(benign_train_7, size=size, replace = True, shuffle = True, axis=0)
    x_8 = rng.choice(benign_train_8, size=size, replace = True, shuffle = True, axis=0)
    x_9 = rng.choice(benign_train_9, size=size, replace = True, shuffle = True, axis=0)


    loader_1 = torch.utils.data.DataLoader(x_1, batch_size=batch_size)
    loader_2 = torch.utils.data.DataLoader(x_2, batch_size=batch_size)
    loader_3 = torch.utils.data.DataLoader(x_3, batch_size=batch_size)
    loader_4 = torch.utils.data.DataLoader(x_4, batch_size=batch_size)
    loader_5 = torch.utils.data.DataLoader(x_5, batch_size=batch_size)
    loader_6 = torch.utils.data.DataLoader(x_6, batch_size=batch_size)
    loader_7 = torch.utils.data.DataLoader(x_7, batch_size=batch_size)
    loader_8 = torch.utils.data.DataLoader(x_8, batch_size=batch_size)
    loader_9 = torch.utils.data.DataLoader(x_9, batch_size=batch_size)

    loaders = {'D1': loader_1,
               'D2': loader_2,
               'D3': loader_3,
               'D4': loader_4,
               'D5': loader_5,
               'D6': loader_6,
               'D7': loader_7,
               'D8': loader_8,
               'D9': loader_9}

    return loaders

"""
    Define Other Training Parameters, Initialize the Model, and Data:
"""
num_clients = 9      # Number of clients
num_selected = 9     # Typically, num_selected is around 30–40% of the num_clients.
num_rounds = 10       # Total number of communication rounds for the global model to train.
epochs = 20           # for train client model
retrain_epochs = 5   # Total number of retraining rounds on the global server after receiving the model weights
                     # from all the clients that participated in the communication round.

global_model = AEModel().to(device) # for example
client_models = [AEModel().to(device) for _ in range(num_selected)]
print(client_models)

for model in client_models:
    model.load_state_dict(global_model.state_dict())

train_loader = train_data_loaders()
print(train_loader)

"""
   Initialize the Optimizer, Data, Devices:
"""
opt = [torch.optim.SGD(model.parameters(), lr = 0.0001, weight_decay=1e-05, momentum=0.9) for model in client_models]
baseline_data = baseline_data()
train_loader = train_data_loaders()
devices = ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9'] # All devices

"""
   Train The Federated Model:
"""
# Train Client Model and Global Model
train_loss_client = []
train_loss_global = []

start_time = time.time()

for r in range(num_rounds):

    print('\nround: ', r + 1)
    client_idx = np.random.permutation(num_clients)[:num_selected] # random pick some clients from all clients for train
    print('client_idx: ', client_idx)
    client_lens = [len(train_loader[devices[idx]]) for idx in client_idx ]
    print(client_lens)

    # Update All Clients:
    loss = 0
    for i in tqdm(range(num_selected)):
        client_syn(client_models[i], global_model)
        loss += client_update(client_models[i], opt[i], train_loader[devices[client_idx[i]]], epochs)
    train_loss_client.append(loss)

    # Retrain the Model on the Global Server:
    loss_retrain = 0
    for i in tqdm(range(num_selected)):
        loss_retrain += client_update(client_models[i], opt[i], baseline_data[devices[client_idx[i]]], retrain_epochs)
    train_loss_global.append(loss_retrain/num_selected)

    # Aggregate all Client Models:
    server_aggregate_M(global_model, client_models, client_lens)

    print("\nclient_loss: ", loss)
    print('global_loss: ', loss_retrain/num_selected)

# save model
torch.save(global_model, 'global_AE.pt')

time_required = time.time() - start_time
print('\nTIME: {}mins'.format(time_required/60))

"""
Load the Trained Global Model - AutoEncoder Model
"""
global_model = torch.load('global_AE.pt')

"""
    Test on Client Devices - First Calculate the Threshold, and then Test on the devices:
"""
performance = {}

# Device 1:
thr_1 = get_tr(global_model, benign_tr_1)
print("D1: ")
performance['D1'] = get_mix_result(global_model, thr_1, mix_data_1, mix_label_1)

print(x)

# Device 2:
thr_2 = get_tr(global_model, benign_tr_2)
print("D2: ")
performance['D2'] = get_mix_result(global_model, thr_2, mix_data_2, mix_label_2)

print(x)

# Device 3:
thr_3 = get_tr(global_model, benign_tr_3)
print("D3: ")
performance['D3'] = get_mix_result(global_model, thr_3, mix_data_3, mix_label_3)

print(x)

# Device 4:
thr_4 = get_tr(global_model, benign_tr_4)
print("D4: ")
performance['D4'] = get_mix_result(global_model, thr_4, mix_data_4, mix_label_4)

print(x)

# Device 5:
thr_5 = get_tr(global_model, benign_tr_5)
print("D5: ")
performance['D5'] = get_mix_result(global_model, thr_5, mix_data_5, mix_label_5)

print(x)

# Device 6:
thr_6 = get_tr(global_model, benign_tr_6)
print("D6: ")
performance['D6'] = get_mix_result(global_model, thr_6, mix_data_6, mix_label_6)

print(x)

# Device 7:
thr_7 = get_tr(global_model, benign_tr_7)
print("D7: ")
performance['D7'] = get_mix_result(global_model, thr_7, mix_data_7, mix_label_7)

print(x)

# Device 8:
thr_8 = get_tr(global_model, benign_tr_8)
print("D8: ")
performance['D8'] = get_mix_result(global_model, thr_8, mix_data_8, mix_label_8)

print(x)

# Device 9:
thr_9 = get_tr(global_model, benign_tr_9)
print("D9: ")
performance['D9'] = get_mix_result(global_model, thr_9, mix_data_9, mix_label_9)

"""
    Create the Data Generator for Main Training Process by Collecting Training Data of each Device,
    Define Other Training Parameters, Initialize the Model, and Data, Initialize the Optimizer, Data, Devices,
    Train the Non-Federated Model, Load Trained Model, and Test on Client's Devices:

    Using Device 5 Optimized Parameters for the AE Model -  Non- Federated Learning:
    {'optimizer': <class 'torch.optim.sgd.SGD'>, 'max_epochs': 20, 'lr': 0.0001, 'batch_size': 128}

"""

"""
   Create the Data Generator for Main Training Process by Collecting Training Data of each Device:
"""
batch_size = 128
def train_data_loaders(batch_size = batch_size):
    data_5 = torch.utils.data.DataLoader(benign_train_5, batch_size = batch_size, shuffle=True)
    return data_5


"""
    Define Other Training Parameters, Initialize the Model, and Data:
"""
epochs = 20           # for train client model

# Model Initialization
model = AEModel().to(device) # for example

train_loader = train_data_loaders()
print(train_loader)

"""
   Initialize the Optimizer, Data, Devices:
"""
opt = torch.optim.SGD(model.parameters(), lr = 0.0001, weight_decay=1e-05, momentum=0.9)
train_loader = train_data_loaders()

"""
   Train The Non- Federated Model:
"""

# Train Non-FL Model
train_loss_client = []

start_time = time.time()

model.train()
for e in range(epochs):
    running_loss = 0.0
    for bx, (data) in enumerate(train_loader):
        output = model(data.float())
        opt.zero_grad()
        loss = nn.MSELoss(reduction='mean')(data.float().to(device), output)
        loss.backward()
        opt.step()
        # print(loss.item())
        running_loss += loss.item()
    # print(running_loss)
    epoch_loss = running_loss/len(train_loader)

# save model
torch.save(model, 'Non_FL_AE.pt')

time_required = time.time() - start_time
print('\nTIME: {}mins'.format(time_required/60))

"""
Load the Trained Non-FL Model - AutoEncoder Model
"""
model = torch.load('Non_FL_AE.pt')

"""
    Test on Client Devices - First Calculate the Threshold, and then Test on the devices:
"""
performance = {}

# Device 1:
thr_1 = get_tr(model, benign_tr_1)
print("D1: ")
performance['D1'] = get_mix_result(model, thr_1, mix_data_1, mix_label_1)

print(x)

# Device 2:
thr_2 = get_tr(model, benign_tr_2)
print("D2: ")
performance['D2'] = get_mix_result(model, thr_2, mix_data_2, mix_label_2)

print(x)

# Device 3:
thr_3 = get_tr(model, benign_tr_3)
print("D3: ")
performance['D3'] = get_mix_result(model, thr_3, mix_data_3, mix_label_3)

print(x)

# Device 4:
thr_4 = get_tr(model, benign_tr_4)
print("D4: ")
performance['D4'] = get_mix_result(model, thr_4, mix_data_4, mix_label_4)

print(x)

# Device 5:
thr_5 = get_tr(model, benign_tr_5)
print("D5: ")
performance['D5'] = get_mix_result(model, thr_5, mix_data_5, mix_label_5)

print(x)

# Device 6:
thr_6 = get_tr(model, benign_tr_6)
print("D6: ")
performance['D6'] = get_mix_result(model, thr_6, mix_data_6, mix_label_6)

print(x)

# Device 7:
thr_7 = get_tr(model, benign_tr_7)
print("D7: ")
performance['D7'] = get_mix_result(model, thr_7, mix_data_7, mix_label_7)

print(x)

# Device 8:
thr_8 = get_tr(model, benign_tr_8)
print("D8: ")
performance['D8'] = get_mix_result(model, thr_8, mix_data_8, mix_label_8)

print(x)

# Device 9:
thr_9 = get_tr(model, benign_tr_9)
print("D9: ")
performance['D9'] = get_mix_result(model, thr_9, mix_data_9, mix_label_9)

"""
    Create the Data Generator for Retraining Process by Picking Baseline Data for each Device,
    Create the Data Generator for Retraining Process by Picking Baseline Data for each Device,
    Define Other Training Parameters, Initialize the Model, and Data, Initialize the Optimizer, Data, Devices,
    Train the Federated Model, Load Trained Model, and Test on Client's Devices:

    Using Device 6 Optimized Parameters for the AE Model:
    {'optimizer': <class 'torch.optim.adam.Adam'>, 'max_epochs': 10, 'lr': 0.001, 'batch_size': 128}
"""

"""
   Create the Data Generator for Main Training Process by Collecting Training Data of each Device:
"""
batch_size = 128
def train_data_loaders(batch_size = batch_size):
    data_1 = torch.utils.data.DataLoader(benign_train_1, batch_size = batch_size, shuffle=True)
    data_2 = torch.utils.data.DataLoader(benign_train_2, batch_size = batch_size, shuffle=True)
    data_3 = torch.utils.data.DataLoader(benign_train_3, batch_size = batch_size, shuffle=True)
    data_4 = torch.utils.data.DataLoader(benign_train_4, batch_size = batch_size, shuffle=True)
    data_5 = torch.utils.data.DataLoader(benign_train_5, batch_size = batch_size, shuffle=True)
    data_6 = torch.utils.data.DataLoader(benign_train_6, batch_size = batch_size, shuffle=True)
    data_7 = torch.utils.data.DataLoader(benign_train_7, batch_size = batch_size, shuffle=True)
    data_8 = torch.utils.data.DataLoader(benign_train_8, batch_size = batch_size, shuffle=True)
    data_9 = torch.utils.data.DataLoader(benign_train_9, batch_size = batch_size, shuffle=True)

    client_loaders = {'D1': data_1,
                      'D2': data_2,
                      'D3': data_3,
                      'D4': data_4,
                      'D5': data_5,
                      'D6': data_6,
                      'D7': data_7,
                      'D8': data_8,
                      'D9': data_9}

    return client_loaders

"""
   Create the Data Generator for Retraining Process by Picking Baseline Data for each Device:
"""
baseline_num = 1000  # choose some data from the train set to retrain the data from trained model
def baseline_data(size = baseline_num, batch_size = batch_size):
    rng = np.random.default_rng()
    x_1 = rng.choice(benign_train_1, size=size, replace = True, shuffle = True, axis=0) # [[[[[[randomly select]]]]]]
    x_2 = rng.choice(benign_train_2, size=size, replace = True, shuffle = True, axis=0)
    x_3 = rng.choice(benign_train_3, size=size, replace = True, shuffle = True, axis=0)
    x_4 = rng.choice(benign_train_4, size=size, replace = True, shuffle = True, axis=0)
    x_5 = rng.choice(benign_train_5, size=size, replace = True, shuffle = True, axis=0)
    x_6 = rng.choice(benign_train_6, size=size, replace = True, shuffle = True, axis=0)
    x_7 = rng.choice(benign_train_7, size=size, replace = True, shuffle = True, axis=0)
    x_8 = rng.choice(benign_train_8, size=size, replace = True, shuffle = True, axis=0)
    x_9 = rng.choice(benign_train_9, size=size, replace = True, shuffle = True, axis=0)


    loader_1 = torch.utils.data.DataLoader(x_1, batch_size=batch_size)
    loader_2 = torch.utils.data.DataLoader(x_2, batch_size=batch_size)
    loader_3 = torch.utils.data.DataLoader(x_3, batch_size=batch_size)
    loader_4 = torch.utils.data.DataLoader(x_4, batch_size=batch_size)
    loader_5 = torch.utils.data.DataLoader(x_5, batch_size=batch_size)
    loader_6 = torch.utils.data.DataLoader(x_6, batch_size=batch_size)
    loader_7 = torch.utils.data.DataLoader(x_7, batch_size=batch_size)
    loader_8 = torch.utils.data.DataLoader(x_8, batch_size=batch_size)
    loader_9 = torch.utils.data.DataLoader(x_9, batch_size=batch_size)

    loaders = {'D1': loader_1,
               'D2': loader_2,
               'D3': loader_3,
               'D4': loader_4,
               'D5': loader_5,
               'D6': loader_6,
               'D7': loader_7,
               'D8': loader_8,
               'D9': loader_9}

    return loaders

"""
    Define Other Training Parameters, Initialize the Model, and Data:
"""
num_clients = 9      # Number of clients
num_selected = 9     # Typically, num_selected is around 30–40% of the num_clients.
num_rounds = 10       # Total number of communication rounds for the global model to train.
epochs = 10           # for train client model
retrain_epochs = 5   # Total number of retraining rounds on the global server after receiving the model weights
                     # from all the clients that participated in the communication round.

global_model = AEModel().to(device) # for example
client_models = [AEModel().to(device) for _ in range(num_selected)]
print(client_models)

for model in client_models:
    model.load_state_dict(global_model.state_dict())

train_loader = train_data_loaders()
print(train_loader)

"""
   Initialize the Optimizer, Data, Devices:
"""
opt = [torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=1e-05) for model in client_models]
baseline_data = baseline_data()
train_loader = train_data_loaders()
devices = ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9'] # All devices

"""
   Train The Federated Model:
"""
# Train Client Model and Global Model
train_loss_client = []
train_loss_global = []

start_time = time.time()

for r in range(num_rounds):

    print('\nround: ', r + 1)
    client_idx = np.random.permutation(num_clients)[:num_selected] # random pick some clients from all clients for train
    print('client_idx: ', client_idx)
    client_lens = [len(train_loader[devices[idx]]) for idx in client_idx ]
    print(client_lens)

    # Update All Clients:
    loss = 0
    for i in tqdm(range(num_selected)):
        client_syn(client_models[i], global_model)
        loss += client_update(client_models[i], opt[i], train_loader[devices[client_idx[i]]], epochs)
    train_loss_client.append(loss)

    # Retrain the Model on the Global Server:
    loss_retrain = 0
    for i in tqdm(range(num_selected)):
        loss_retrain += client_update(client_models[i], opt[i], baseline_data[devices[client_idx[i]]], retrain_epochs)
    train_loss_global.append(loss_retrain/num_selected)

    # Aggregate all Client Models:
    server_aggregate_M(global_model, client_models, client_lens)

    print("\nclient_loss: ", loss)
    print('global_loss: ', loss_retrain/num_selected)

# save model
torch.save(global_model, 'global_AE.pt')

time_required = time.time() - start_time
print('\nTIME: {}mins'.format(time_required/60))

"""
Load the Trained Global Model - AutoEncoder Model
"""
global_model = torch.load('global_AE.pt')

"""
    Test on Client Devices - First Calculate the Threshold, and then Test on the devices:
"""
performance = {}

# Device 1:
thr_1 = get_tr(global_model, benign_tr_1)
print("D1: ")
performance['D1'] = get_mix_result(global_model, thr_1, mix_data_1, mix_label_1)

print(x)

# Device 2:
thr_2 = get_tr(global_model, benign_tr_2)
print("D2: ")
performance['D2'] = get_mix_result(global_model, thr_2, mix_data_2, mix_label_2)

print(x)

# Device 3:
thr_3 = get_tr(global_model, benign_tr_3)
print("D3: ")
performance['D3'] = get_mix_result(global_model, thr_3, mix_data_3, mix_label_3)

print(x)

# Device 4:
thr_4 = get_tr(global_model, benign_tr_4)
print("D4: ")
performance['D4'] = get_mix_result(global_model, thr_4, mix_data_4, mix_label_4)

print(x)

# Device 5:
thr_5 = get_tr(global_model, benign_tr_5)
print("D5: ")
performance['D5'] = get_mix_result(global_model, thr_5, mix_data_5, mix_label_5)

print(x)

# Device 6:
thr_6 = get_tr(global_model, benign_tr_6)
print("D6: ")
performance['D6'] = get_mix_result(global_model, thr_6, mix_data_6, mix_label_6)

print(x)

# Device 7:
thr_7 = get_tr(global_model, benign_tr_7)
print("D7: ")
performance['D7'] = get_mix_result(global_model, thr_7, mix_data_7, mix_label_7)

print(x)

# Device 8:
thr_8 = get_tr(global_model, benign_tr_8)
print("D8: ")
performance['D8'] = get_mix_result(global_model, thr_8, mix_data_8, mix_label_8)

print(x)

# Device 9:
thr_9 = get_tr(global_model, benign_tr_9)
print("D9: ")
performance['D9'] = get_mix_result(global_model, thr_9, mix_data_9, mix_label_9)

"""
    Create the Data Generator for Main Training Process by Collecting Training Data of each Device,
    Define Other Training Parameters, Initialize the Model, and Data, Initialize the Optimizer, Data, Devices,
    Train the Non-Federated Model, Load Trained Model, and Test on Client's Devices:

    Using Device 6 Optimized Parameters for the AE Model -  Non- Federated Learning:
    {'optimizer': <class 'torch.optim.adam.Adam'>, 'max_epochs': 10, 'lr': 0.001, 'batch_size': 128}

"""

"""
   Create the Data Generator for Main Training Process by Collecting Training Data of each Device:
"""
batch_size = 128
def train_data_loaders(batch_size = batch_size):
    data_6 = torch.utils.data.DataLoader(benign_train_6, batch_size = batch_size, shuffle=True)
    return data_6


"""
    Define Other Training Parameters, Initialize the Model, and Data:
"""
epochs = 10           # for train client model

# Model Initialization
model = AEModel().to(device) # for example

train_loader = train_data_loaders()
print(train_loader)

"""
   Initialize the Optimizer, Data, Devices:
"""
opt = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=1e-05)
train_loader = train_data_loaders()

"""
   Train The Non- Federated Model:
"""

# Train Non-FL Model
train_loss_client = []

start_time = time.time()

model.train()
for e in range(epochs):
    running_loss = 0.0
    for bx, (data) in enumerate(train_loader):
        output = model(data.float())
        opt.zero_grad()
        loss = nn.MSELoss(reduction='mean')(data.float().to(device), output)
        loss.backward()
        opt.step()
        # print(loss.item())
        running_loss += loss.item()
    # print(running_loss)
    epoch_loss = running_loss/len(train_loader)

# save model
torch.save(model, 'Non_FL_AE.pt')

time_required = time.time() - start_time
print('\nTIME: {}mins'.format(time_required/60))

"""
Load the Trained Non-FL Model - AutoEncoder Model
"""
model = torch.load('Non_FL_AE.pt')

"""
    Test on Client Devices - First Calculate the Threshold, and then Test on the devices:
"""
performance = {}

# Device 1:
thr_1 = get_tr(model, benign_tr_1)
print("D1: ")
performance['D1'] = get_mix_result(model, thr_1, mix_data_1, mix_label_1)

print(x)

# Device 2:
thr_2 = get_tr(model, benign_tr_2)
print("D2: ")
performance['D2'] = get_mix_result(model, thr_2, mix_data_2, mix_label_2)

print(x)

# Device 3:
thr_3 = get_tr(model, benign_tr_3)
print("D3: ")
performance['D3'] = get_mix_result(model, thr_3, mix_data_3, mix_label_3)

print(x)

# Device 4:
thr_4 = get_tr(model, benign_tr_4)
print("D4: ")
performance['D4'] = get_mix_result(model, thr_4, mix_data_4, mix_label_4)

print(x)

# Device 5:
thr_5 = get_tr(model, benign_tr_5)
print("D5: ")
performance['D5'] = get_mix_result(model, thr_5, mix_data_5, mix_label_5)

print(x)

# Device 6:
thr_6 = get_tr(model, benign_tr_6)
print("D6: ")
performance['D6'] = get_mix_result(model, thr_6, mix_data_6, mix_label_6)

print(x)

# Device 7:
thr_7 = get_tr(model, benign_tr_7)
print("D7: ")
performance['D7'] = get_mix_result(model, thr_7, mix_data_7, mix_label_7)

print(x)

# Device 8:
thr_8 = get_tr(model, benign_tr_8)
print("D8: ")
performance['D8'] = get_mix_result(model, thr_8, mix_data_8, mix_label_8)

print(x)

# Device 9:
thr_9 = get_tr(model, benign_tr_9)
print("D9: ")
performance['D9'] = get_mix_result(model, thr_9, mix_data_9, mix_label_9)

"""
    Create the Data Generator for Retraining Process by Picking Baseline Data for each Device,
    Create the Data Generator for Retraining Process by Picking Baseline Data for each Device,
    Define Other Training Parameters, Initialize the Model, and Data, Initialize the Optimizer, Data, Devices,
    Train the Federated Model, Load Trained Model, and Test on Client's Devices:

    Using Device 7 Optimized Parameters for the AE Model:
    {'optimizer': <class 'torch.optim.sgd.SGD'>, 'max_epochs': 40, 'lr': 0.0001, 'batch_size': 256}
"""

"""
   Create the Data Generator for Main Training Process by Collecting Training Data of each Device:
"""
batch_size = 256
def train_data_loaders(batch_size = batch_size):
    data_1 = torch.utils.data.DataLoader(benign_train_1, batch_size = batch_size, shuffle=True)
    data_2 = torch.utils.data.DataLoader(benign_train_2, batch_size = batch_size, shuffle=True)
    data_3 = torch.utils.data.DataLoader(benign_train_3, batch_size = batch_size, shuffle=True)
    data_4 = torch.utils.data.DataLoader(benign_train_4, batch_size = batch_size, shuffle=True)
    data_5 = torch.utils.data.DataLoader(benign_train_5, batch_size = batch_size, shuffle=True)
    data_6 = torch.utils.data.DataLoader(benign_train_6, batch_size = batch_size, shuffle=True)
    data_7 = torch.utils.data.DataLoader(benign_train_7, batch_size = batch_size, shuffle=True)
    data_8 = torch.utils.data.DataLoader(benign_train_8, batch_size = batch_size, shuffle=True)
    data_9 = torch.utils.data.DataLoader(benign_train_9, batch_size = batch_size, shuffle=True)

    client_loaders = {'D1': data_1,
                      'D2': data_2,
                      'D3': data_3,
                      'D4': data_4,
                      'D5': data_5,
                      'D6': data_6,
                      'D7': data_7,
                      'D8': data_8,
                      'D9': data_9}

    return client_loaders

"""
   Create the Data Generator for Retraining Process by Picking Baseline Data for each Device:
"""
baseline_num = 1000  # choose some data from the train set to retrain the data from trained model
def baseline_data(size = baseline_num, batch_size = batch_size):
    rng = np.random.default_rng()
    x_1 = rng.choice(benign_train_1, size=size, replace = True, shuffle = True, axis=0) # [[[[[[randomly select]]]]]]
    x_2 = rng.choice(benign_train_2, size=size, replace = True, shuffle = True, axis=0)
    x_3 = rng.choice(benign_train_3, size=size, replace = True, shuffle = True, axis=0)
    x_4 = rng.choice(benign_train_4, size=size, replace = True, shuffle = True, axis=0)
    x_5 = rng.choice(benign_train_5, size=size, replace = True, shuffle = True, axis=0)
    x_6 = rng.choice(benign_train_6, size=size, replace = True, shuffle = True, axis=0)
    x_7 = rng.choice(benign_train_7, size=size, replace = True, shuffle = True, axis=0)
    x_8 = rng.choice(benign_train_8, size=size, replace = True, shuffle = True, axis=0)
    x_9 = rng.choice(benign_train_9, size=size, replace = True, shuffle = True, axis=0)


    loader_1 = torch.utils.data.DataLoader(x_1, batch_size=batch_size)
    loader_2 = torch.utils.data.DataLoader(x_2, batch_size=batch_size)
    loader_3 = torch.utils.data.DataLoader(x_3, batch_size=batch_size)
    loader_4 = torch.utils.data.DataLoader(x_4, batch_size=batch_size)
    loader_5 = torch.utils.data.DataLoader(x_5, batch_size=batch_size)
    loader_6 = torch.utils.data.DataLoader(x_6, batch_size=batch_size)
    loader_7 = torch.utils.data.DataLoader(x_7, batch_size=batch_size)
    loader_8 = torch.utils.data.DataLoader(x_8, batch_size=batch_size)
    loader_9 = torch.utils.data.DataLoader(x_9, batch_size=batch_size)

    loaders = {'D1': loader_1,
               'D2': loader_2,
               'D3': loader_3,
               'D4': loader_4,
               'D5': loader_5,
               'D6': loader_6,
               'D7': loader_7,
               'D8': loader_8,
               'D9': loader_9}

    return loaders

"""
    Define Other Training Parameters, Initialize the Model, and Data:
"""
num_clients = 9      # Number of clients
num_selected = 9     # Typically, num_selected is around 30–40% of the num_clients.
num_rounds = 10       # Total number of communication rounds for the global model to train.
epochs = 40           # for train client model
retrain_epochs = 5   # Total number of retraining rounds on the global server after receiving the model weights
                     # from all the clients that participated in the communication round.

global_model = AEModel().to(device) # for example
client_models = [AEModel().to(device) for _ in range(num_selected)]
print(client_models)

for model in client_models:
    model.load_state_dict(global_model.state_dict())

train_loader = train_data_loaders()
print(train_loader)

"""
   Initialize the Optimizer, Data, Devices:
"""
opt = [torch.optim.SGD(model.parameters(), lr = 0.0001, weight_decay=1e-05, momentum=0.9) for model in client_models]
baseline_data = baseline_data()
train_loader = train_data_loaders()
devices = ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9'] # All devices

"""
   Train The Federated Model:
"""
# Train Client Model and Global Model
train_loss_client = []
train_loss_global = []

start_time = time.time()

for r in range(num_rounds):

    print('\nround: ', r + 1)
    client_idx = np.random.permutation(num_clients)[:num_selected] # random pick some clients from all clients for train
    print('client_idx: ', client_idx)
    client_lens = [len(train_loader[devices[idx]]) for idx in client_idx ]
    print(client_lens)

    # Update All Clients:
    loss = 0
    for i in tqdm(range(num_selected)):
        client_syn(client_models[i], global_model)
        loss += client_update(client_models[i], opt[i], train_loader[devices[client_idx[i]]], epochs)
    train_loss_client.append(loss)

    # Retrain the Model on the Global Server:
    loss_retrain = 0
    for i in tqdm(range(num_selected)):
        loss_retrain += client_update(client_models[i], opt[i], baseline_data[devices[client_idx[i]]], retrain_epochs)
    train_loss_global.append(loss_retrain/num_selected)

    # Aggregate all Client Models:
    server_aggregate_M(global_model, client_models, client_lens)

    print("\nclient_loss: ", loss)
    print('global_loss: ', loss_retrain/num_selected)

# save model
torch.save(global_model, 'global_AE.pt')

time_required = time.time() - start_time
print('\nTIME: {}mins'.format(time_required/60))

"""
Load the Trained Global Model - AutoEncoder Model
"""
global_model = torch.load('global_AE.pt')

"""
    Test on Client Devices - First Calculate the Threshold, and then Test on the devices:
"""
performance = {}

# Device 1:
thr_1 = get_tr(global_model, benign_tr_1)
print("D1: ")
performance['D1'] = get_mix_result(global_model, thr_1, mix_data_1, mix_label_1)

print(x)

# Device 2:
thr_2 = get_tr(global_model, benign_tr_2)
print("D2: ")
performance['D2'] = get_mix_result(global_model, thr_2, mix_data_2, mix_label_2)

print(x)

# Device 3:
thr_3 = get_tr(global_model, benign_tr_3)
print("D3: ")
performance['D3'] = get_mix_result(global_model, thr_3, mix_data_3, mix_label_3)

print(x)

# Device 4:
thr_4 = get_tr(global_model, benign_tr_4)
print("D4: ")
performance['D4'] = get_mix_result(global_model, thr_4, mix_data_4, mix_label_4)

print(x)

# Device 5:
thr_5 = get_tr(global_model, benign_tr_5)
print("D5: ")
performance['D5'] = get_mix_result(global_model, thr_5, mix_data_5, mix_label_5)

print(x)

# Device 6:
thr_6 = get_tr(global_model, benign_tr_6)
print("D6: ")
performance['D6'] = get_mix_result(global_model, thr_6, mix_data_6, mix_label_6)

print(x)

# Device 7:
thr_7 = get_tr(global_model, benign_tr_7)
print("D7: ")
performance['D7'] = get_mix_result(global_model, thr_7, mix_data_7, mix_label_7)

print(x)

# Device 8:
thr_8 = get_tr(global_model, benign_tr_8)
print("D8: ")
performance['D8'] = get_mix_result(global_model, thr_8, mix_data_8, mix_label_8)

print(x)

# Device 9:
thr_9 = get_tr(global_model, benign_tr_9)
print("D9: ")
performance['D9'] = get_mix_result(global_model, thr_9, mix_data_9, mix_label_9)

"""
    Create the Data Generator for Main Training Process by Collecting Training Data of each Device,
    Define Other Training Parameters, Initialize the Model, and Data, Initialize the Optimizer, Data, Devices,
    Train the Non-Federated Model, Load Trained Model, and Test on Client's Devices:

    Using Device 7 Optimized Parameters for the AE Model -  Non- Federated Learning:
    {'optimizer': <class 'torch.optim.sgd.SGD'>, 'max_epochs': 40, 'lr': 0.0001, 'batch_size': 256}

"""

"""
   Create the Data Generator for Main Training Process by Collecting Training Data of each Device:
"""
batch_size = 256
def train_data_loaders(batch_size = batch_size):
    data_7 = torch.utils.data.DataLoader(benign_train_7, batch_size = batch_size, shuffle=True)
    return data_7


"""
    Define Other Training Parameters, Initialize the Model, and Data:
"""
epochs = 40           # for train client model

# Model Initialization
model = AEModel().to(device) # for example

train_loader = train_data_loaders()
print(train_loader)

"""
   Initialize the Optimizer, Data, Devices:
"""
opt = torch.optim.SGD(model.parameters(), lr = 0.0001, weight_decay=1e-05, momentum=0.9)
train_loader = train_data_loaders()

"""
   Train The Non- Federated Model:
"""

# Train Non-FL Model
train_loss_client = []

start_time = time.time()

model.train()
for e in range(epochs):
    running_loss = 0.0
    for bx, (data) in enumerate(train_loader):
        output = model(data.float())
        opt.zero_grad()
        loss = nn.MSELoss(reduction='mean')(data.float().to(device), output)
        loss.backward()
        opt.step()
        # print(loss.item())
        running_loss += loss.item()
    # print(running_loss)
    epoch_loss = running_loss/len(train_loader)

# save model
torch.save(model, 'Non_FL_AE.pt')

time_required = time.time() - start_time
print('\nTIME: {}mins'.format(time_required/60))

"""
Load the Trained Non-FL Model - AutoEncoder Model
"""
model = torch.load('Non_FL_AE.pt')

"""
    Test on Client Devices - First Calculate the Threshold, and then Test on the devices:
"""
performance = {}

# Device 1:
thr_1 = get_tr(model, benign_tr_1)
print("D1: ")
performance['D1'] = get_mix_result(model, thr_1, mix_data_1, mix_label_1)

print(x)

# Device 2:
thr_2 = get_tr(model, benign_tr_2)
print("D2: ")
performance['D2'] = get_mix_result(model, thr_2, mix_data_2, mix_label_2)

print(x)

# Device 3:
thr_3 = get_tr(model, benign_tr_3)
print("D3: ")
performance['D3'] = get_mix_result(model, thr_3, mix_data_3, mix_label_3)

print(x)

# Device 4:
thr_4 = get_tr(model, benign_tr_4)
print("D4: ")
performance['D4'] = get_mix_result(model, thr_4, mix_data_4, mix_label_4)

print(x)

# Device 5:
thr_5 = get_tr(model, benign_tr_5)
print("D5: ")
performance['D5'] = get_mix_result(model, thr_5, mix_data_5, mix_label_5)

print(x)

# Device 6:
thr_6 = get_tr(model, benign_tr_6)
print("D6: ")
performance['D6'] = get_mix_result(model, thr_6, mix_data_6, mix_label_6)

print(x)

# Device 7:
thr_7 = get_tr(model, benign_tr_7)
print("D7: ")
performance['D7'] = get_mix_result(model, thr_7, mix_data_7, mix_label_7)

print(x)

# Device 8:
thr_8 = get_tr(model, benign_tr_8)
print("D8: ")
performance['D8'] = get_mix_result(model, thr_8, mix_data_8, mix_label_8)

print(x)

# Device 9:
thr_9 = get_tr(model, benign_tr_9)
print("D9: ")
performance['D9'] = get_mix_result(model, thr_9, mix_data_9, mix_label_9)

"""
    Create the Data Generator for Retraining Process by Picking Baseline Data for each Device,
    Create the Data Generator for Retraining Process by Picking Baseline Data for each Device,
    Define Other Training Parameters, Initialize the Model, and Data, Initialize the Optimizer, Data, Devices,
    Train the Federated Model, Load Trained Model, and Test on Client's Devices:

    Using Device 8 Optimized Parameters for the AE Model:
    {'optimizer': <class 'torch.optim.sgd.SGD'>, 'max_epochs': 40, 'lr': 0.001, 'batch_size': 64}
"""

"""
   Create the Data Generator for Main Training Process by Collecting Training Data of each Device:
"""
batch_size = 64
def train_data_loaders(batch_size = batch_size):
    data_1 = torch.utils.data.DataLoader(benign_train_1, batch_size = batch_size, shuffle=True)
    data_2 = torch.utils.data.DataLoader(benign_train_2, batch_size = batch_size, shuffle=True)
    data_3 = torch.utils.data.DataLoader(benign_train_3, batch_size = batch_size, shuffle=True)
    data_4 = torch.utils.data.DataLoader(benign_train_4, batch_size = batch_size, shuffle=True)
    data_5 = torch.utils.data.DataLoader(benign_train_5, batch_size = batch_size, shuffle=True)
    data_6 = torch.utils.data.DataLoader(benign_train_6, batch_size = batch_size, shuffle=True)
    data_7 = torch.utils.data.DataLoader(benign_train_7, batch_size = batch_size, shuffle=True)
    data_8 = torch.utils.data.DataLoader(benign_train_8, batch_size = batch_size, shuffle=True)
    data_9 = torch.utils.data.DataLoader(benign_train_9, batch_size = batch_size, shuffle=True)

    client_loaders = {'D1': data_1,
                      'D2': data_2,
                      'D3': data_3,
                      'D4': data_4,
                      'D5': data_5,
                      'D6': data_6,
                      'D7': data_7,
                      'D8': data_8,
                      'D9': data_9}

    return client_loaders

"""
   Create the Data Generator for Retraining Process by Picking Baseline Data for each Device:
"""
baseline_num = 1000  # choose some data from the train set to retrain the data from trained model
def baseline_data(size = baseline_num, batch_size = batch_size):
    rng = np.random.default_rng()
    x_1 = rng.choice(benign_train_1, size=size, replace = True, shuffle = True, axis=0) # [[[[[[randomly select]]]]]]
    x_2 = rng.choice(benign_train_2, size=size, replace = True, shuffle = True, axis=0)
    x_3 = rng.choice(benign_train_3, size=size, replace = True, shuffle = True, axis=0)
    x_4 = rng.choice(benign_train_4, size=size, replace = True, shuffle = True, axis=0)
    x_5 = rng.choice(benign_train_5, size=size, replace = True, shuffle = True, axis=0)
    x_6 = rng.choice(benign_train_6, size=size, replace = True, shuffle = True, axis=0)
    x_7 = rng.choice(benign_train_7, size=size, replace = True, shuffle = True, axis=0)
    x_8 = rng.choice(benign_train_8, size=size, replace = True, shuffle = True, axis=0)
    x_9 = rng.choice(benign_train_9, size=size, replace = True, shuffle = True, axis=0)


    loader_1 = torch.utils.data.DataLoader(x_1, batch_size=batch_size)
    loader_2 = torch.utils.data.DataLoader(x_2, batch_size=batch_size)
    loader_3 = torch.utils.data.DataLoader(x_3, batch_size=batch_size)
    loader_4 = torch.utils.data.DataLoader(x_4, batch_size=batch_size)
    loader_5 = torch.utils.data.DataLoader(x_5, batch_size=batch_size)
    loader_6 = torch.utils.data.DataLoader(x_6, batch_size=batch_size)
    loader_7 = torch.utils.data.DataLoader(x_7, batch_size=batch_size)
    loader_8 = torch.utils.data.DataLoader(x_8, batch_size=batch_size)
    loader_9 = torch.utils.data.DataLoader(x_9, batch_size=batch_size)

    loaders = {'D1': loader_1,
               'D2': loader_2,
               'D3': loader_3,
               'D4': loader_4,
               'D5': loader_5,
               'D6': loader_6,
               'D7': loader_7,
               'D8': loader_8,
               'D9': loader_9}

    return loaders

"""
    Define Other Training Parameters, Initialize the Model, and Data:
"""
num_clients = 9      # Number of clients
num_selected = 9     # Typically, num_selected is around 30–40% of the num_clients.
num_rounds = 10       # Total number of communication rounds for the global model to train.
epochs = 40           # for train client model
retrain_epochs = 5   # Total number of retraining rounds on the global server after receiving the model weights
                     # from all the clients that participated in the communication round.

global_model = AEModel().to(device) # for example
client_models = [AEModel().to(device) for _ in range(num_selected)]
print(client_models)

for model in client_models:
    model.load_state_dict(global_model.state_dict())

train_loader = train_data_loaders()
print(train_loader)

"""
   Initialize the Optimizer, Data, Devices:
"""
opt = [torch.optim.SGD(model.parameters(), lr = 0.001, weight_decay=1e-05, momentum=0.9) for model in client_models]
baseline_data = baseline_data()
train_loader = train_data_loaders()
devices = ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9'] # All devices

"""
   Train The Federated Model:
"""
# Train Client Model and Global Model
train_loss_client = []
train_loss_global = []

start_time = time.time()

for r in range(num_rounds):

    print('\nround: ', r + 1)
    client_idx = np.random.permutation(num_clients)[:num_selected] # random pick some clients from all clients for train
    print('client_idx: ', client_idx)
    client_lens = [len(train_loader[devices[idx]]) for idx in client_idx ]
    print(client_lens)

    # Update All Clients:
    loss = 0
    for i in tqdm(range(num_selected)):
        client_syn(client_models[i], global_model)
        loss += client_update(client_models[i], opt[i], train_loader[devices[client_idx[i]]], epochs)
    train_loss_client.append(loss)

    # Retrain the Model on the Global Server:
    loss_retrain = 0
    for i in tqdm(range(num_selected)):
        loss_retrain += client_update(client_models[i], opt[i], baseline_data[devices[client_idx[i]]], retrain_epochs)
    train_loss_global.append(loss_retrain/num_selected)

    # Aggregate all Client Models:
    server_aggregate_M(global_model, client_models, client_lens)

    print("\nclient_loss: ", loss)
    print('global_loss: ', loss_retrain/num_selected)

# save model
torch.save(global_model, 'global_AE.pt')

time_required = time.time() - start_time
print('\nTIME: {}mins'.format(time_required/60))

"""
Load the Trained Global Model - AutoEncoder Model
"""
global_model = torch.load('global_AE.pt')

"""
    Test on Client Devices - First Calculate the Threshold, and then Test on the devices:
"""
performance = {}

# Device 1:
thr_1 = get_tr(global_model, benign_tr_1)
print("D1: ")
performance['D1'] = get_mix_result(global_model, thr_1, mix_data_1, mix_label_1)

print(x)

# Device 2:
thr_2 = get_tr(global_model, benign_tr_2)
print("D2: ")
performance['D2'] = get_mix_result(global_model, thr_2, mix_data_2, mix_label_2)

print(x)

# Device 3:
thr_3 = get_tr(global_model, benign_tr_3)
print("D3: ")
performance['D3'] = get_mix_result(global_model, thr_3, mix_data_3, mix_label_3)

print(x)

# Device 4:
thr_4 = get_tr(global_model, benign_tr_4)
print("D4: ")
performance['D4'] = get_mix_result(global_model, thr_4, mix_data_4, mix_label_4)

print(x)

# Device 5:
thr_5 = get_tr(global_model, benign_tr_5)
print("D5: ")
performance['D5'] = get_mix_result(global_model, thr_5, mix_data_5, mix_label_5)

print(x)

# Device 6:
thr_6 = get_tr(global_model, benign_tr_6)
print("D6: ")
performance['D6'] = get_mix_result(global_model, thr_6, mix_data_6, mix_label_6)

print(x)

# Device 7:
thr_7 = get_tr(global_model, benign_tr_7)
print("D7: ")
performance['D7'] = get_mix_result(global_model, thr_7, mix_data_7, mix_label_7)

print(x)

# Device 8:
thr_8 = get_tr(global_model, benign_tr_8)
print("D8: ")
performance['D8'] = get_mix_result(global_model, thr_8, mix_data_8, mix_label_8)

print(x)

# Device 9:
thr_9 = get_tr(global_model, benign_tr_9)
print("D9: ")
performance['D9'] = get_mix_result(global_model, thr_9, mix_data_9, mix_label_9)

"""
    Create the Data Generator for Main Training Process by Collecting Training Data of each Device,
    Define Other Training Parameters, Initialize the Model, and Data, Initialize the Optimizer, Data, Devices,
    Train the Non-Federated Model, Load Trained Model, and Test on Client's Devices:

    Using Device 8 Optimized Parameters for the AE Model -  Non- Federated Learning:
    {'optimizer': <class 'torch.optim.sgd.SGD'>, 'max_epochs': 40, 'lr': 0.001, 'batch_size': 64}

"""

"""
   Create the Data Generator for Main Training Process by Collecting Training Data of each Device:
"""
batch_size = 64
def train_data_loaders(batch_size = batch_size):
    data_8 = torch.utils.data.DataLoader(benign_train_8, batch_size = batch_size, shuffle=True)
    return data_8


"""
    Define Other Training Parameters, Initialize the Model, and Data:
"""
epochs = 40           # for train client model

# Model Initialization
model = AEModel().to(device) # for example

train_loader = train_data_loaders()
print(train_loader)

"""
   Initialize the Optimizer, Data, Devices:
"""
opt = torch.optim.SGD(model.parameters(), lr = 0.001, weight_decay=1e-05, momentum=0.9)
train_loader = train_data_loaders()

"""
   Train The Non- Federated Model:
"""

# Train Non-FL Model
train_loss_client = []

start_time = time.time()

model.train()
for e in range(epochs):
    running_loss = 0.0
    for bx, (data) in enumerate(train_loader):
        output = model(data.float())
        opt.zero_grad()
        loss = nn.MSELoss(reduction='mean')(data.float().to(device), output)
        loss.backward()
        opt.step()
        # print(loss.item())
        running_loss += loss.item()
    # print(running_loss)
    epoch_loss = running_loss/len(train_loader)

# save model
torch.save(model, 'Non_FL_AE.pt')

time_required = time.time() - start_time
print('\nTIME: {}mins'.format(time_required/60))

"""
Load the Trained Non-FL Model - AutoEncoder Model
"""
model = torch.load('Non_FL_AE.pt')

"""
    Test on Client Devices - First Calculate the Threshold, and then Test on the devices:
"""
performance = {}

# Device 1:
thr_1 = get_tr(model, benign_tr_1)
print("D1: ")
performance['D1'] = get_mix_result(model, thr_1, mix_data_1, mix_label_1)

print(x)

# Device 2:
thr_2 = get_tr(model, benign_tr_2)
print("D2: ")
performance['D2'] = get_mix_result(model, thr_2, mix_data_2, mix_label_2)

print(x)

# Device 3:
thr_3 = get_tr(model, benign_tr_3)
print("D3: ")
performance['D3'] = get_mix_result(model, thr_3, mix_data_3, mix_label_3)

print(x)

# Device 4:
thr_4 = get_tr(model, benign_tr_4)
print("D4: ")
performance['D4'] = get_mix_result(model, thr_4, mix_data_4, mix_label_4)

print(x)

# Device 5:
thr_5 = get_tr(model, benign_tr_5)
print("D5: ")
performance['D5'] = get_mix_result(model, thr_5, mix_data_5, mix_label_5)

print(x)

# Device 6:
thr_6 = get_tr(model, benign_tr_6)
print("D6: ")
performance['D6'] = get_mix_result(model, thr_6, mix_data_6, mix_label_6)

print(x)

# Device 7:
thr_7 = get_tr(model, benign_tr_7)
print("D7: ")
performance['D7'] = get_mix_result(model, thr_7, mix_data_7, mix_label_7)

print(x)

# Device 8:
thr_8 = get_tr(model, benign_tr_8)
print("D8: ")
performance['D8'] = get_mix_result(model, thr_8, mix_data_8, mix_label_8)

print(x)

# Device 9:
thr_9 = get_tr(model, benign_tr_9)
print("D9: ")
performance['D9'] = get_mix_result(model, thr_9, mix_data_9, mix_label_9)

"""
    Create the Data Generator for Retraining Process by Picking Baseline Data for each Device,
    Create the Data Generator for Retraining Process by Picking Baseline Data for each Device,
    Define Other Training Parameters, Initialize the Model, and Data, Initialize the Optimizer, Data, Devices,
    Train the Federated Model, Load Trained Model, and Test on Client's Devices:

    Using Device 9 Optimized Parameters for the AE Model:
    {'optimizer': <class 'torch.optim.adam.Adam'>, 'max_epochs': 10, 'lr': 0.001, 'batch_size': 64}
"""

"""
   Create the Data Generator for Main Training Process by Collecting Training Data of each Device:
"""
batch_size = 64
def train_data_loaders(batch_size = batch_size):
    data_1 = torch.utils.data.DataLoader(benign_train_1, batch_size = batch_size, shuffle=True)
    data_2 = torch.utils.data.DataLoader(benign_train_2, batch_size = batch_size, shuffle=True)
    data_3 = torch.utils.data.DataLoader(benign_train_3, batch_size = batch_size, shuffle=True)
    data_4 = torch.utils.data.DataLoader(benign_train_4, batch_size = batch_size, shuffle=True)
    data_5 = torch.utils.data.DataLoader(benign_train_5, batch_size = batch_size, shuffle=True)
    data_6 = torch.utils.data.DataLoader(benign_train_6, batch_size = batch_size, shuffle=True)
    data_7 = torch.utils.data.DataLoader(benign_train_7, batch_size = batch_size, shuffle=True)
    data_8 = torch.utils.data.DataLoader(benign_train_8, batch_size = batch_size, shuffle=True)
    data_9 = torch.utils.data.DataLoader(benign_train_9, batch_size = batch_size, shuffle=True)

    client_loaders = {'D1': data_1,
                      'D2': data_2,
                      'D3': data_3,
                      'D4': data_4,
                      'D5': data_5,
                      'D6': data_6,
                      'D7': data_7,
                      'D8': data_8,
                      'D9': data_9}

    return client_loaders

"""
   Create the Data Generator for Retraining Process by Picking Baseline Data for each Device:
"""
baseline_num = 1000  # choose some data from the train set to retrain the data from trained model
def baseline_data(size = baseline_num, batch_size = batch_size):
    rng = np.random.default_rng()
    x_1 = rng.choice(benign_train_1, size=size, replace = True, shuffle = True, axis=0) # [[[[[[randomly select]]]]]]
    x_2 = rng.choice(benign_train_2, size=size, replace = True, shuffle = True, axis=0)
    x_3 = rng.choice(benign_train_3, size=size, replace = True, shuffle = True, axis=0)
    x_4 = rng.choice(benign_train_4, size=size, replace = True, shuffle = True, axis=0)
    x_5 = rng.choice(benign_train_5, size=size, replace = True, shuffle = True, axis=0)
    x_6 = rng.choice(benign_train_6, size=size, replace = True, shuffle = True, axis=0)
    x_7 = rng.choice(benign_train_7, size=size, replace = True, shuffle = True, axis=0)
    x_8 = rng.choice(benign_train_8, size=size, replace = True, shuffle = True, axis=0)
    x_9 = rng.choice(benign_train_9, size=size, replace = True, shuffle = True, axis=0)


    loader_1 = torch.utils.data.DataLoader(x_1, batch_size=batch_size)
    loader_2 = torch.utils.data.DataLoader(x_2, batch_size=batch_size)
    loader_3 = torch.utils.data.DataLoader(x_3, batch_size=batch_size)
    loader_4 = torch.utils.data.DataLoader(x_4, batch_size=batch_size)
    loader_5 = torch.utils.data.DataLoader(x_5, batch_size=batch_size)
    loader_6 = torch.utils.data.DataLoader(x_6, batch_size=batch_size)
    loader_7 = torch.utils.data.DataLoader(x_7, batch_size=batch_size)
    loader_8 = torch.utils.data.DataLoader(x_8, batch_size=batch_size)
    loader_9 = torch.utils.data.DataLoader(x_9, batch_size=batch_size)

    loaders = {'D1': loader_1,
               'D2': loader_2,
               'D3': loader_3,
               'D4': loader_4,
               'D5': loader_5,
               'D6': loader_6,
               'D7': loader_7,
               'D8': loader_8,
               'D9': loader_9}

    return loaders

"""
    Define Other Training Parameters, Initialize the Model, and Data:
"""
num_clients = 9      # Number of clients
num_selected = 9     # Typically, num_selected is around 30–40% of the num_clients.
num_rounds = 10       # Total number of communication rounds for the global model to train.
epochs = 10           # for train client model
retrain_epochs = 5   # Total number of retraining rounds on the global server after receiving the model weights
                     # from all the clients that participated in the communication round.

global_model = AEModel().to(device) # for example
client_models = [AEModel().to(device) for _ in range(num_selected)]
print(client_models)

for model in client_models:
    model.load_state_dict(global_model.state_dict())

train_loader = train_data_loaders()
print(train_loader)

"""
   Initialize the Optimizer, Data, Devices:
"""
opt = [torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=1e-05) for model in client_models]
baseline_data = baseline_data()
train_loader = train_data_loaders()
devices = ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9'] # All devices

"""
   Train The Federated Model:
"""
# Train Client Model and Global Model
train_loss_client = []
train_loss_global = []

start_time = time.time()

for r in range(num_rounds):

    print('\nround: ', r + 1)
    client_idx = np.random.permutation(num_clients)[:num_selected] # random pick some clients from all clients for train
    print('client_idx: ', client_idx)
    client_lens = [len(train_loader[devices[idx]]) for idx in client_idx ]
    print(client_lens)

    # Update All Clients:
    loss = 0
    for i in tqdm(range(num_selected)):
        client_syn(client_models[i], global_model)
        loss += client_update(client_models[i], opt[i], train_loader[devices[client_idx[i]]], epochs)
    train_loss_client.append(loss)

    # Retrain the Model on the Global Server:
    loss_retrain = 0
    for i in tqdm(range(num_selected)):
        loss_retrain += client_update(client_models[i], opt[i], baseline_data[devices[client_idx[i]]], retrain_epochs)
    train_loss_global.append(loss_retrain/num_selected)

    # Aggregate all Client Models:
    server_aggregate_M(global_model, client_models, client_lens)

    print("\nclient_loss: ", loss)
    print('global_loss: ', loss_retrain/num_selected)

# save model
torch.save(global_model, 'global_AE.pt')

time_required = time.time() - start_time
print('\nTIME: {}mins'.format(time_required/60))

"""
Load the Trained Global Model - AutoEncoder Model
"""
global_model = torch.load('global_AE.pt')

"""
    Test on Client Devices - First Calculate the Threshold, and then Test on the devices:
"""
performance = {}

# Device 1:
thr_1 = get_tr(global_model, benign_tr_1)
print("D1: ")
performance['D1'] = get_mix_result(global_model, thr_1, mix_data_1, mix_label_1)

print(x)

# Device 2:
thr_2 = get_tr(global_model, benign_tr_2)
print("D2: ")
performance['D2'] = get_mix_result(global_model, thr_2, mix_data_2, mix_label_2)

print(x)

# Device 3:
thr_3 = get_tr(global_model, benign_tr_3)
print("D3: ")
performance['D3'] = get_mix_result(global_model, thr_3, mix_data_3, mix_label_3)

print(x)

# Device 4:
thr_4 = get_tr(global_model, benign_tr_4)
print("D4: ")
performance['D4'] = get_mix_result(global_model, thr_4, mix_data_4, mix_label_4)

print(x)

# Device 5:
thr_5 = get_tr(global_model, benign_tr_5)
print("D5: ")
performance['D5'] = get_mix_result(global_model, thr_5, mix_data_5, mix_label_5)

print(x)

# Device 6:
thr_6 = get_tr(global_model, benign_tr_6)
print("D6: ")
performance['D6'] = get_mix_result(global_model, thr_6, mix_data_6, mix_label_6)

print(x)

# Device 7:
thr_7 = get_tr(global_model, benign_tr_7)
print("D7: ")
performance['D7'] = get_mix_result(global_model, thr_7, mix_data_7, mix_label_7)

print(x)

# Device 8:
thr_8 = get_tr(global_model, benign_tr_8)
print("D8: ")
performance['D8'] = get_mix_result(global_model, thr_8, mix_data_8, mix_label_8)

print(x)

# Device 9:
thr_9 = get_tr(global_model, benign_tr_9)
print("D9: ")
performance['D9'] = get_mix_result(global_model, thr_9, mix_data_9, mix_label_9)

"""
    Create the Data Generator for Main Training Process by Collecting Training Data of each Device,
    Define Other Training Parameters, Initialize the Model, and Data, Initialize the Optimizer, Data, Devices,
    Train the Non-Federated Model, Load Trained Model, and Test on Client's Devices:

    Using Device 9 Optimized Parameters for the AE Model -  Non- Federated Learning:
    {'optimizer': <class 'torch.optim.adam.Adam'>, 'max_epochs': 10, 'lr': 0.001, 'batch_size': 64}

"""

"""
   Create the Data Generator for Main Training Process by Collecting Training Data of each Device:
"""
batch_size = 64
def train_data_loaders(batch_size = batch_size):
    data_9 = torch.utils.data.DataLoader(benign_train_9, batch_size = batch_size, shuffle=True)
    return data_9


"""
    Define Other Training Parameters, Initialize the Model, and Data:
"""
epochs = 10           # for train client model

# Model Initialization
model = AEModel().to(device) # for example

train_loader = train_data_loaders()
print(train_loader)

"""
   Initialize the Optimizer, Data, Devices:
"""
opt = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=1e-05)
train_loader = train_data_loaders()

"""
   Train The Non- Federated Model:
"""

# Train Non-FL Model
train_loss_client = []

start_time = time.time()

model.train()
for e in range(epochs):
    running_loss = 0.0
    for bx, (data) in enumerate(train_loader):
        output = model(data.float())
        opt.zero_grad()
        loss = nn.MSELoss(reduction='mean')(data.float().to(device), output)
        loss.backward()
        opt.step()
        # print(loss.item())
        running_loss += loss.item()
    # print(running_loss)
    epoch_loss = running_loss/len(train_loader)

# save model
torch.save(model, 'Non_FL_AE.pt')

time_required = time.time() - start_time
print('\nTIME: {}mins'.format(time_required/60))

"""
Load the Trained Non-FL Model - AutoEncoder Model
"""
model = torch.load('Non_FL_AE.pt')

"""
    Test on Client Devices - First Calculate the Threshold, and then Test on the devices:
"""
performance = {}

# Device 1:
thr_1 = get_tr(model, benign_tr_1)
print("D1: ")
performance['D1'] = get_mix_result(model, thr_1, mix_data_1, mix_label_1)

print(x)

# Device 2:
thr_2 = get_tr(model, benign_tr_2)
print("D2: ")
performance['D2'] = get_mix_result(model, thr_2, mix_data_2, mix_label_2)

print(x)

# Device 3:
thr_3 = get_tr(model, benign_tr_3)
print("D3: ")
performance['D3'] = get_mix_result(model, thr_3, mix_data_3, mix_label_3)

print(x)

# Device 4:
thr_4 = get_tr(model, benign_tr_4)
print("D4: ")
performance['D4'] = get_mix_result(model, thr_4, mix_data_4, mix_label_4)

print(x)

# Device 5:
thr_5 = get_tr(model, benign_tr_5)
print("D5: ")
performance['D5'] = get_mix_result(model, thr_5, mix_data_5, mix_label_5)

print(x)

# Device 6:
thr_6 = get_tr(model, benign_tr_6)
print("D6: ")
performance['D6'] = get_mix_result(model, thr_6, mix_data_6, mix_label_6)

print(x)

# Device 7:
thr_7 = get_tr(model, benign_tr_7)
print("D7: ")
performance['D7'] = get_mix_result(model, thr_7, mix_data_7, mix_label_7)

print(x)

# Device 8:
thr_8 = get_tr(model, benign_tr_8)
print("D8: ")
performance['D8'] = get_mix_result(model, thr_8, mix_data_8, mix_label_8)

print(x)

# Device 9:
thr_9 = get_tr(model, benign_tr_9)
print("D9: ")
performance['D9'] = get_mix_result(model, thr_9, mix_data_9, mix_label_9)